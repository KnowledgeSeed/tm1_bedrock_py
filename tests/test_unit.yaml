# Utility: MDX query parsing functions
test_get_cube_name_from_mdx:
  -
    mdx_query:
      "FROM [Cost and FTE by Groups]"

test_mdx_filter_to_dictionary:
  -
    mdx_query:
      "WHERE 
          (
           [Versions].[Versions].[Base Plan], 
           [Lineitems Cost and FTE by Groups].[Lineitems Cost and FTE by Groups].[FTE],
           [Measures Cost and FTE by Groups].[Measures Cost and FTE by Groups].[Value]
          )"
  -
    mdx_query:
      "WHERE 
          (
           [Versions].[Base Plan], 
           [Lineitems Cost and FTE by Groups].[Lineitems Cost and FTE by Groups].[FTE],
           [Measures Cost and FTE by Groups].[Measures Cost and FTE by Groups].[Value]
          )"
  -
    mdx_query:
      ""

test_get_kwargs_dict_from_set_mdx_list_success:
  -
    set_mdx_list: [
      "{[Periods].[Periods].[2024]}",
      "{Tm1FilterbyLevel({Tm1DrillDownMember({[Versions].[All Versions]}, ALL, RECURSIVE)}, 0)}"
    ]
    expected_kwargs: {
      "periods": "{[Periods].[Periods].[2024]}",
      "versions": "{Tm1FilterbyLevel({Tm1DrillDownMember({[Versions].[All Versions]}, ALL, RECURSIVE)}, 0)}"
    }
  -
    set_mdx_list: [
      "{[ Product Category ] . [ Members ] . [ All ] }",
      "{ [TIME DIM   ].[H1].[Q1]}"
    ]
    expected_kwargs: {
      "productcategory": "{[ Product Category ] . [ Members ] . [ All ] }",
      "timedim": "{ [TIME DIM   ].[H1].[Q1]}"
    }
  -
    # Underscores and numbers in dimension name
    set_mdx_list: [
      "{[Sales_Region].[North America]}",
      "{[Scenario 1].[Actual]}"
    ]
    expected_kwargs: {
      "sales_region": "{[Sales_Region].[North America]}",
      "scenario1": "{[Scenario 1].[Actual]}"
    }
  -
    set_mdx_list: ["Tm1SubsetAll([Periods])", "Tm1SubsetAll([Versions].[Financial])"]
    expected_kwargs: {
      "periods": "Tm1SubsetAll([Periods])",
      "versions": "Tm1SubsetAll([Versions].[Financial])"
    }
  -
    set_mdx_list: [ "Tm1FilterByLevel(Tm1SubsetAll([Periods]), 0)",
                    "Tm1FilterByLevel(Tm1DrillDownmember([Versions].[Versions].[All Versions], ALL, RECURSIVE), 0)"]
    expected_kwargs: {
      "periods": "Tm1FilterByLevel(Tm1SubsetAll([Periods]), 0)",
      "versions": "Tm1FilterByLevel(Tm1DrillDownmember([Versions].[Versions].[All Versions], ALL, RECURSIVE), 0)"
    }

test__get_kwargs_dict_from_set_mdx_list_fail:
  -
    mdx_expressions: ["Invalid string without braces"]
    expected_exception: "ValueError"
  -
    mdx_expressions: ["{[ValidDim].[Member]}", "completely invalid"]
    expected_exception: "ValueError"
  -
    mdx_expressions: ["{[AnotherDim].[Member]}", null]
    expected_exception: "TypeError"
  -
    mdx_expressions: [
      "{[Geography].[City].[London]}",
      "This is just a string",
      "{[Account].[Net Income]}",
      "{Malformed Structure]"
    ]
    expected_exception: "ValueError"
  -
    mdx_expressions: [
      "Just a plain string",
      "Another string without the pattern",
      "12345"
    ]
    expected_exception: "ValueError"
  -
    mdx_expressions: [
      "{Dimension.Member}",
      "[Dimension].[Member]",
      "{[ Dimension Name ]",
      " { [ Dim ] } extra text"
    ]
    expected_exception: "ValueError"
  -
    mdx_expressions: [
      "{Dimension.Member}",
      "[Dimension].[Member]",
      "{[ Dimension Name ]",
      "{ [ ] }",
      "{ [ No Closing Bracket "
    ]
    expected_exception: "ValueError"
  -
    mdx_expressions: ["", "   "]
    expected_exception: "ValueError"
  -
    mdx_expressions: [None, "{[ValidDim].[Value]}"]
    expected_exception: "ValueError"
  -
    mdx_expressions: ["{[ValidDim].[Value]}", "{[ValidDim].[Value]}"]
    expected_exception: "ValueError"
  -
    mdx_expressions: []
    expected_exception: "ValueError"
  -
    mdx_expressions: ["{[].[element]}"]
    expected_exception: "ValueError"
  -
    mdx_expressions: [ "Tm1FilterBylevel(.[element]), 0)}" ]
    expected_exception: "ValueError"
  -
    mdx_expressions: "invalidlist"
    expected_exception: "TypeError"

test_get_dimensions_from_set_mdx_list_success:
  -
    mdx_sets: [
        "{[Periods].[Periods].[2024]}",
        "{ [Versions] .[ Actual ] }"
      ]
    expected_dimensions: ["Periods", "Versions"]
  -
    mdx_sets: ["Tm1FilterByLevel(Tm1SubsetAll([Periods]), 0)",
               "Tm1FilterByLevel(Tm1DrillDownmember([Versions].[Versions].[All Versions], ALL, RECURSIVE), 0)"]
    expected_dimensions: ["Periods", "Versions"]
  -
    mdx_sets: [ "Tm1SubsetAll([  Periods    ])", "Tm1SubsetAll([Versions].[Financial])" ]
    expected_dimensions: ["Periods", "Versions"]


test_get_dimensions_from_set_mdx_list_failure:
  -
    mdx_sets: []
    expected_exception: "ValueError"
    expected_message_part: "Set mdx list cannot be empty"
  -
    mdx_sets: [
      "{ [ Measures ] . [ Sales ] }",
      "{ [ Measures ] . [ Units ] }"
    ]
    expected_exception: "ValueError"
    expected_message_part: "Duplicate set mdxs for a dimension"
  -
    mdx_sets: "not a list"
    expected_exception: "TypeError"
    expected_message_part: "Expected mdx_sets to be a list"
  -
    mdx_sets: 123
    expected_exception: "TypeError"
    expected_message_part: "Expected mdx_sets to be a list"
  -
    mdx_sets: ["[Dim1]", 123, "[Dim3]"]
    expected_exception: "TypeError"
    expected_message_part: "Expected elements of mdx_sets to be strings"
  -
    mdx_sets: [!!binary "Ynl0ZXM=", "{[Dim2]}"]
    expected_exception: "TypeError"
    expected_message_part: "Expected elements of mdx_sets to be strings"
  -
    mdx_sets: [ "invalidmdx", "invalidmdx2" ]
    expected_exception: "ValueError"
    expected_message_part: "The mdx invalidmdx is invalid"
  -
    mdx_sets: ["{[ValidDim].[Value]}", "invalidmdx" ]
    expected_exception: "ValueError"
    expected_message_part: "The mdx invalidmdx is invalid"
  -
    mdx_sets: [ "{[].[Value]}", "{[ValidDim].[Value]}" ]
    expected_exception: "ValueError"
    expected_message_part: "The mdx {[].[Value]} is invalid"



test_generate_cartesian_product_success:
  -
    list_of_lists: [["a", "b"], [1, 2]]
    expected_product: [["a", 1], ["a", 2], ["b", 1], ["b", 2]]
  -
    list_of_lists: [[True, False], ["x"]]
    expected_product: [[True, "x"], [False, "x"]]
  -
    list_of_lists: [["one"], ["two"], ["three"]]
    expected_product: [["one", "two", "three"]]
  -
    list_of_lists: [[1, 2], [], [3, 4]]
    expected_product: []
  -
    list_of_lists: [["a", "b"]]
    expected_product: [["a"], ["b"]]
  -
    list_of_lists: []
    expected_product: []
  -
    list_of_lists: null
    expected_product: []
  -
    list_of_lists: [[10], [null, 20], [30]]
    expected_product: [[10, null, 30], [10, 20, 30]]


test_generate_cartesian_product_failure:
  -
    list_of_lists: "not a list"
    expected_exception: "TypeError"
    expected_message_part: "Input must be a list"
  -
    list_of_lists: 123
    expected_exception: "TypeError"
    expected_message_part: "Input must be a list"
  -
    list_of_lists: {"a": 1}
    expected_exception: "TypeError"
    expected_message_part: "Input must be a list"
  -
    list_of_lists: [[1, 2], 3, [4, 5]]
    expected_exception: "TypeError"
    expected_message_part: "'int' object is not iterable"
  -
    list_of_lists: [["a", "b"], ["c", "d"], 100]
    expected_exception: "TypeError"
    expected_message_part: "'int' object is not iterable"
  -
    list_of_lists: [[1], [2], null, [4]]
    expected_exception: "TypeError"
    expected_message_part: "'NoneType' object is not iterable"


test_generate_element_lists_from_set_mdx_list_success:
  -
    set_mdx_list: [
      "{TM1FILTERBYLEVEL({DRILLDOWNMEMBER({[Periods].[Periods].[2024]}, {[Periods].[Periods].[2024]})}, 0)}",
      "{[Versions].[Versions].[Base Plan], [Versions].[Versions].[Budget]}"
    ]
    expected_result: [
      ["202401", "202402", "202403", "202404", "202405", "202406", "202407", "202408", "202409", "202410", "202411", "202412"],
      ["Base Plan", "Budget"]
    ]

test_generate_element_lists_from_set_mdx_list_failure:
  -
    use_none_service: true
    set_mdx_list: ["Query1"]
    expected_exception: "ValueError"
    expected_message_part: "tm1_service cannot be None"
  -
    use_none_service: false
    set_mdx_list: "not a list"
    expected_exception: "TypeError"
    expected_message_part: "Expected set_mdx_list to be a list"
  -
    use_none_service: false
    set_mdx_list: ["{[Versions].[Versions].[Base Plan], [Versions].[Versions].[Budget]}", 123]
    expected_exception: "TypeError"
    expected_message_part: "Expected elements of set_mdx_list to be strings"
  -
    use_none_service: false
    set_mdx_list: ["MDX_FOR_INDEX_ERROR"]
    expected_exception: "TM1pyRestException"
    expected_message_part: "Text: '{\"error\":{\"code\":\"501\",\"message\":\"Could not create MDX Set.\"}}' - Status Code: 400 - Reason: 'Bad Request' - Headers: {'Content-Length': '79', 'Connection': 'keep-alive', 'Content-Encoding': 'gzip', 'Cache-Control': 'no-cache', 'Content-Type': 'application/json; charset=utf-8', 'OData-Version': '4.0'}"


test_extract_mdx_components:
  -
    input_mdx: |
      SELECT
      {[Periods].[element]}
      ON COLUMNS,
      {something}
      ON ROWS
      FROM [cubename]
    expected_set_mdx_list: ["{{[Periods].[element]}}", "{{something}}"]
  -
    input_mdx: |
      SELECT
      {[Periods].[element]}
      ON 0,
      {something}
      ON 1,
          Tm1SubsetAll([dim3])
      ON 2
      FROM [cubename]
    expected_set_mdx_list: ["{{[Periods].[element]}}", "{{something}}", "{Tm1SubsetAll([dim3])}"]
  -
    input_mdx: |
      SELECT
      {[Periods].[Year].[element]} * {[Versions].[element]}
      ON 0
      FROM [cubename]
    expected_set_mdx_list: ["{{[Periods].[Year].[element]}}", "{{[Versions].[element]}}"]
  -
    input_mdx: |
      SELECT
      {[Periods].[Year].[element]}
      ON 0
      FROM [cubename]
    expected_set_mdx_list: [ "{{[Periods].[Year].[element]}}"]
  -
    input_mdx: |
      SELECT
        {[Periods].[element]}
      ON COLUMNS,
          {something}
      ON ROWS
      FROM [cubename]
      WHERE(
          [dim].[element],
          [dim2].[element]
      )
    expected_set_mdx_list: ["{{[Periods].[element]}}", "{{something}}", "{[dim].[element]}", "{[dim2].[element]}"]
  -
    input_mdx: |
      SELECT
      {[Periods].[element]}
      ON COLUMNS,
      {something}
      ON ROWS
      FROM [cubename]
    expected_set_mdx_list: ["{{[Periods].[element]}}", "{{something}}"]
  -
    input_mdx: |
      SELECT
      {[Periods].[element]} * {[Version].[element2]} * {[Lineitem].[element3]}
      ON COLUMNS,
      {something} 
        * Tm1SubsetAll([dim3])
      ON ROWS
      FROM [cubename]
    expected_set_mdx_list: ["{{[Periods].[element]}}",
                            "{{[Version].[element2]}}",
                            "{{[Lineitem].[element3]}}",
                            "{{something}}",
                            "{Tm1SubsetAll([dim3])}"]
  -
    input_mdx: |
      SELECT
        {[Periods].[element]} 
                             * {[Version].[element2]}
      ON 0,
      {something(something)} * Tm1SubsetAll([dim3])
      ON 1,
      {another}
      ON 2
      FROM [cubename]
      WHERE([dim].[element],
             [dim2].[element],
                     [dim3].[hier3].[element]
      )
    expected_set_mdx_list: ["{{[Periods].[element]}}",
                            "{{[Version].[element2]}}",
                            "{{something(something)}}",
                            "{Tm1SubsetAll([dim3])}",
                            "{{another}}",
                            "{[dim].[element]}",
                            "{[dim2].[element]}",
                            "{[dim3].[hier3].[element]}"]
  -
    input_mdx: |
      SELECT
          {[Periods].[element]}     *  
          {[Version].[element2]}    
      ON 0,
          {something}   
      ON 1
      FROM [cubename]
      WHERE(
          [dim].[element]   ,
            [dim2].[element]  
      )
    expected_set_mdx_list: [
      "{{[Periods].[element]}}", "{{[Version].[element2]}}", "{{something}}", "{[dim].[element]}", "{[dim2].[element]}"
    ]
  -
    input_mdx: |
      SELECT
      {[Periods].[element,with comma]}
      ON COLUMNS,
      {[Versions].[element2,,with comma]} * {[Versions].[element3,,,with comma]}
      FROM [cubename]
    expected_set_mdx_list: [
      "{{[Periods].[element,withcomma]}}",
      "{{[Versions].[element2,,withcomma]}}",
      "{{[Versions].[element3,,,withcomma]}}"
    ]
  -
    input_mdx: |
      SELECT
        {[Periods].[element]} * {[Version].[Version].[Actual]}
      ON COLUMNS,
            {  something } 
                  * Tm1SubsetAll([OrgUnit])
      ON ROWS
      FROM [cubename]
      WHERE(
          [dim].[element1,withcomma ],
                [dim2].[element2,,withcommas],
          [dim3].[hierarchy,withcomma].[element3,withcomma]
      )
    expected_set_mdx_list: [
      "{{[Periods].[element]}}",
      "{{[Version].[Version].[Actual]}}",
      "{{something}}",
      "{Tm1SubsetAll([OrgUnit])}",
      "{[dim].[element1,withcomma]}",
      "{[dim2].[element2,,withcommas]}",
      "{[dim3].[hierarchy,withcomma].[element3,withcomma]}"
    ]
  -
    input_mdx: |
      SELECT
        {[Periods].[element]} * {[Version].[Version].[Actual]}
      ON 0,
            {  something } 
                  * Tm1SubsetAll(  [  OrgUnit])
      ON 1,
          {[Employee].[Key Account Managers].[All KAMs]}
      ON 2
      FROM [cubename]
      WHERE(
          [dim].[element1,withcomma ],
                [dim2].[element2, ,  withcommas  ]       ,
          [dim3].[hierarchy,withcomma].[element3,withcomma]
      )
    expected_set_mdx_list: [
      "{{[Periods].[element]}}",
      "{{[Version].[Version].[Actual]}}",
      "{{something}}",
      "{Tm1SubsetAll([OrgUnit])}",
      "{{[Employee].[KeyAccountManagers].[AllKAMs]}}",
      "{[dim].[element1,withcomma]}",
      "{[dim2].[element2,,withcommas]}",
      "{[dim3].[hierarchy,withcomma].[element3,withcomma]}"
    ]
  -
    input_mdx: |
      SELECT
      {[Periods].[element], [Periods].[element2]}
      ON COLUMNS,
      {[Version].[element], [Version].[element2], [Version].[element3], [Version].[element4]}
      ON ROWS
      FROM [cubename]
    expected_set_mdx_list: [
      "{{[Periods].[element],[Periods].[element2]}}",
      "{{[Version].[element],[Version].[element2],[Version].[element3],[Version].[element4]}}"
    ]
  -
    input_mdx: |
      SELECT
        {[Periods].[element],    [Periods   ].[element2]} * {[Version].[Version].[Actual],  
            [Version].[Version ].[FC],
      
      [Version].[Version ].[Budget]
      }
      ON 0,
            {  something } 
                  * Tm1SubsetAll(  [  OrgUnit])
      ON 1,
          {[Employee].[Key Account Managers].[All KAMs]}
      ON 2
      FROM [cubename]
      WHERE(
          [dim].[element1,withcomma ],
                [dim2].[element2, ,  withcommas  ]       ,
          [dim3].[hierarchy,withcomma].[element3,withcomma]
      )
    expected_set_mdx_list: [
      "{{[Periods].[element],[Periods].[element2]}}",
      "{{[Version].[Version].[Actual],[Version].[Version].[FC],[Version].[Version].[Budget]}}",
      "{{something}}",
      "{Tm1SubsetAll([OrgUnit])}",
      "{{[Employee].[KeyAccountManagers].[AllKAMs]}}",
      "{[dim].[element1,withcomma]}",
      "{[dim2].[element2,,withcommas]}",
      "{[dim3].[hierarchy,withcomma].[element3,withcomma]}"
    ]
  -
    input_mdx: |
        SELECT NON   EMPTY
        {[Periods].[element], [Periods].[element2]}
        ON COLUMNS,
          NON EMPTY
        {[Version].[element], [Version].[element2], [Version].[element3], [Version].[element4]}
        ON ROWS
        FROM [cubename]
    expected_set_mdx_list: [
      "{{[Periods].[element],[Periods].[element2]}}",
      "{{[Version].[element],[Version].[element2],[Version].[element3],[Version].[element4]}}"
    ]
  -
    input_mdx: |
      SELECT
      NON EMPTY
        {[Periods].[element],    [Periods   ].[element2]} * {[Version].[Version].[Actual],  
            [Version].[Version ].[FC],

      [Version].[Version ].[Budget]
      }
      ON 0,
        NON  EMPTY
            {  something } 
                  * Tm1SubsetAll(  [  OrgUnit])
      ON 1,
      NON EMPTY
          {[Employee].[Key Account Managers].[All KAMs]}
      ON 2
      FROM [cubename]
      WHERE(
          [dim].[element1,withcomma ],
                [dim2].[element2, ,  withcommas  ]       ,
          [dim3].[hierarchy,withcomma].[element3,withcomma]
      )
    expected_set_mdx_list: [
      "{{[Periods].[element],[Periods].[element2]}}",
      "{{[Version].[Version].[Actual],[Version].[Version].[FC],[Version].[Version].[Budget]}}",
      "{{something}}",
      "{Tm1SubsetAll([OrgUnit])}",
      "{{[Employee].[KeyAccountManagers].[AllKAMs]}}",
      "{[dim].[element1,withcomma]}",
      "{[dim2].[element2,,withcommas]}",
      "{[dim3].[hierarchy,withcomma].[element3,withcomma]}"
    ]

test_utility_float_casting_values:
  -
    input_value: 12.48
    expected_value: 12.48
  -
    input_value: "12.48"
    expected_value: 12.48
  -
    input_value: "12,48"
    expected_value: 12.48
  -
    input_value: 12
    expected_value: 12.0
  -
    input_value: "test string 123"
    expected_value: "test string 123"

test_utility_float_casting_types:
  -
    input_value: 12.48
    expected_type: "<class 'numpy.float64'>"
  -
    input_value: "12.48"
    expected_type: "<class 'numpy.float64'>"
  -
    input_value: "12,48"
    expected_type: "<class 'numpy.float64'>"
  -
    input_value: 12
    expected_type: "<class 'numpy.float64'>"
  -
    input_value: "test string 123"
    expected_type: "<class 'str'>"

test_add_nonempty_to_mdx_all_modes:
  -
    input_mdx: "
    SELECT
    { [Measures].[Sales Amount],
    [Measures].[Tax Amount] } ON COLUMNS,
    { [Date].[Fiscal].[Fiscal Year].&[2002],
    [Date].[Fiscal].[Fiscal Year].&[2003] } ON ROWS
    FROM [Adventure Works]
    WHERE ( [Sales Territory].[Southwest] )
    "
    expected_mdx: "
    SELECT
      NON EMPTY { [Measures].[Sales Amount],
    [Measures].[Tax Amount] } ON COLUMNS,
      NON EMPTY { [Date].[Fiscal].[Fiscal Year].&[2002],
    [Date].[Fiscal].[Fiscal Year].&[2003] } ON ROWS
    FROM [Adventure Works]
    WHERE ( [Sales Territory].[Southwest] )
    "
  -
    input_mdx: "
    SELECT
        { [Measures].[Sales Amount] } ON 0,
        { [Date].[Fiscal Year].Members } ON 1
    FROM [My Cube]
    "
    expected_mdx: "
    SELECT
          NON EMPTY { [Measures].[Sales Amount] } ON 0,
          NON EMPTY { [Date].[Fiscal Year].Members } ON 1
    FROM [My Cube]
    "
  -
    input_mdx: "
    SELECT
    { [ Measures ].[ Amount ] } ON 0,
    { [Date].[Year].Members } ON 1,
    { [Region].[Country].Members } ON 2
    FROM [Sales Cube]
    "
    expected_mdx: "
    SELECT
     NON EMPTY { [Measures].[Amount] } ON 0,
     NON EMPTY { [Date].[Year].Members } ON 1,
     NON EMPTY { [Region].[Country].Members } ON 2
    FROM [Sales Cube]
    "
  -
    input_mdx: "
    SELECT
    
    { [ A ] }    ON   COLUMNS   ,
    
    { [B] }
    ON
    1
    FROM    [Cube]
    "
    expected_mdx: "
    SELECT
    
     NON EMPTY { [A] }    ON   COLUMNS   ,
    
     NON EMPTY { [B] }
    ON
    1
    FROM    [Cube]
    "
  -
    input_mdx: "
    SELECT
     NON EMPTY { [Measures].[Sales Amount] } ON COLUMNS,
    { [Date].[Fiscal Year].Members } ON ROWS
    FROM [Adventure Works]
    "
    expected_mdx: "
    SELECT
     NON EMPTY { [Measures].[Sales Amount] } ON COLUMNS,
     NON EMPTY { [Date].[Fiscal Year].Members } ON ROWS
    FROM [Adventure Works]
    "
  -
    input_mdx: "
    SELECT
    { [Measures].[Sales Amount] } ON 0,
     NON EMPTY { [Date].[Fiscal Year].Members } ON 1
    FROM [Adventure Works]
    "
    expected_mdx: "
    SELECT
     NON EMPTY { [Measures].[Sales Amount] } ON 0,
     NON EMPTY { [Date].[Fiscal Year].Members } ON 1
    FROM [Adventure Works]
    "
  -
    input_mdx: "
    SELECT
     NON EMPTY { [Measures].[Sales Amount] } ON COLUMNS,
     NON EMPTY { [Date].[Fiscal Year].Members } ON ROWS
    FROM [Adventure Works]
    "
    expected_mdx: "
    SELECT
     NON EMPTY { [Measures].[Sales Amount] } ON COLUMNS,
     NON EMPTY { [Date].[Fiscal Year].Members } ON ROWS
    FROM [Adventure Works]
    "
  -
    input_mdx: "
    select
    { [ A ] } on columns,
    { [B] } On rOwS
    from [Cube]
    "
    expected_mdx: "
    select
     NON EMPTY { [A] } on columns,
     NON EMPTY { [B] } On rOwS
    from [Cube]
    "
  -
    input_mdx: "
    SELECT
     non empty { [Measures].[Sales Amount] } ON COLUMNS, -- Should not add another
    { [ Date ].[ Fiscal Year ].Members } ON ROWS
    FROM [Adventure Works]
    "
    expected_mdx: "
    SELECT
     non empty { [Measures].[Sales Amount] } ON COLUMNS, -- Should not add another
     NON EMPTY { [Date].[Fiscal Year].Members } ON ROWS
    FROM [Adventure Works]
    "
  -
    input_mdx: "
    WITH MEMBER Measures.X AS 1
    SELECT
    { [ Measures ].[ Sales Amount ] } ON 0,
    { [Date].[Fiscal Year].Members } ON 1
    FROM [My Cube]
    "
    expected_mdx: "
    WITH MEMBER Measures.X AS 1
    SELECT
     NON EMPTY { [Measures].[Sales Amount] } ON 0,
     NON EMPTY { [Date].[Fiscal Year].Members } ON 1
    FROM [My Cube]
    "
  -
    input_mdx: "
    SELECT
    { Filter({[Product].[Product].Members}, [Measures].[Sales] > 100) } ON 0,
    { [Customer].[City].Members } ON 1
    FROM [Sales]
    "
    expected_mdx: "
    SELECT
     NON EMPTY { Filter({[Product].[Product].Members}, [Measures].[Sales] > 100) } ON 0,
     NON EMPTY { [Customer].[City].Members } ON 1
    FROM [Sales]
    "
  -
    input_mdx: "
    SELECT
    FROM [Adventure Works]
    WHERE ( [Sales Territory].[Southwest] )
    "
    expected_mdx: "
    SELECT
    FROM [Adventure Works]
    WHERE ( [Sales Territory].[Southwest] )
    "
  -
    input_mdx: " SELECT
                       NON EMPTY
                       {Tm1SubsetAll([ESRS Main])}
                       * {Tm1SubsetAll([ESRS Details 1])}
                       * {Tm1SubsetAll([ESRS Details 2])}
                       * {Tm1SubsetAll([ESRS Geography])}
                       * {Tm1SubsetAll([Custom 1])}
                       * {Tm1SubsetAll([Custom 2])}
                   ON ROWS,
                       NON EMPTY
                       {Tm1SubsetAll([Analogic ESRS Mapping Measure])}
                   ON COLUMNS
                   FROM [Analogic ESRS Mapping]
                   WHERE (
                       [Year].[Year].[2023],
                       [Entity].[Entity].[Entity NA],
                       [Version].[Version].[Actual]
                   )"
    expected_mdx: " SELECT
                       NON EMPTY
                       {Tm1SubsetAll([ESRS Main])}
                       * {Tm1SubsetAll([ESRS Details 1])}
                       * {Tm1SubsetAll([ESRS Details 2])}
                       * {Tm1SubsetAll([ESRS Geography])}
                       * {Tm1SubsetAll([Custom 1])}
                       * {Tm1SubsetAll([Custom 2])}
                   ON ROWS,
                       NON EMPTY
                       {Tm1SubsetAll([Analogic ESRS Mapping Measure])}
                   ON COLUMNS
                   FROM [Analogic ESRS Mapping]
                   WHERE (
                       [Year].[Year].[2023],
                       [Entity].[Entity].[Entity NA],
                       [Version].[Version].[Actual]
                   )"

test_all_leaves_identifiers_to_dataframe:
  -
    dimname: "Groups"
    expected: {
      "Groups": ["Group_1", "Marketing and Management",
                 "Group_2", "Finance and HR",
                 "Group_3", "Project A",
                 "Group_4", "Project B",
                 "Group_5", "Project C"]
    }

test_rename_columns_with_reference:
  -
    input_df: {
      "lineitemsales": ["Revenue", "Revenue"],
      "organizationunits": ["abc123", "xzy456"],
      "periods": ["202402", "202403"],
      "versions": ["Actuals", "Actuals"],
      "Value": [1000, 1100]
    }
    input_list: ["Periods", "Versions", "Organization Units", "Lineitem Sales"]
    expected_df: {
      "Lineitem Sales": [ "Revenue", "Revenue" ],
      "Organization Units": [ "abc123", "xzy456" ],
      "Periods": [ "202402", "202403" ],
      "Versions": [ "Actuals", "Actuals" ],
      "Value": [ 1000, 1100 ]
    }

test_normalize_dict_strings:
  -
    input_dict: {
      "Outer Key": {"Middle  Key": {"Inner Key": "Value"}},
      "List Key": ["Value 1", " Value 2", "Value 3 "],
      "String Key": "String Value Test"
    }
    expected_dict: {
      "outerkey": { "middlekey": { "innerkey": "value" } },
      "listkey": [ "value1", "value2", "value3" ],
      "stringkey": "stringvaluetest"
    }

test_normalize_dataframe_strings:
  -
    input_df: {
      "Lineitem Sales": [ "Revenue", "Quantity" ],
      "Organization Units": [ "abc123", "xzy456" ],
      "Periods": [ "202402", "202403" ],
      "Versions": [ "Actuals", "Actuals" ],
      "Value": [ 1000, 1100 ]
    }
    expected_df: {
      "lineitemsales": ["revenue", "quantity"],
      "organizationunits": ["abc123", "xzy456"],
      "periods": ["202402", "202403"],
      "versions": ["actuals", "actuals"],
      "value": [1000, 1100]
    }

# Utility: Cube metadata collection using input MDXs and/or other cubes
test_tm1_cube_object_metadata_collect_based_on_cube_name_success:
  -
    cube_name: "Group Employee"
  -
    cube_name: "Employee Organization"

test_tm1_cube_object_metadata_collect_based_on_cube_name_fail:
  -
    cube_name: "Group Employees"
    exception: TM1pyRestException
  -
    cube_name: ["Group Employee"]
    exception: TM1pyRestException
  -
    cube_name: 101
    exception: TM1pyRestException
  -
    cube_name: ""
    exception: TM1pyRestException

test_tm1_cube_object_metadata_collect_based_on_mdx_name_success:
  -
    data_mdx:
      "SELECT 
           {[Periods].[Periods].[202301],[Periods].[Periods].[202302],[Periods].[Periods].[202303],
           [Periods].[Periods].[202304],[Periods].[Periods].[202305],[Periods].[Periods].[202306],
           [Periods].[Periods].[202307],[Periods].[Periods].[202308],[Periods].[Periods].[202309],
           [Periods].[Periods].[202310],[Periods].[Periods].[202311],[Periods].[Periods].[202312]}  
          ON COLUMNS , 
           {[Groups].[Groups].Members}
           * {[Employees].[Employees].Members} 
          ON ROWS 
        FROM [Cost and FTE by Groups] 
        WHERE 
          (
           [Versions].[Versions].[Base Plan], 
           [Lineitems Cost and FTE by Groups].[Lineitems Cost and FTE by Groups].[FTE],
           [Measures Cost and FTE by Groups].[Measures Cost and FTE by Groups].[Value]
          )"

test_tm1_cube_object_metadata_collect_based_on_mdx_name_fail:
    -
      data_mdx: ""
      exception: TM1pyRestException
    -
      data_mdx: 1011
      exception: TypeError
    -
      data_mdx: "Group Employee"
      exception: ValueError
    -
      data_mdx: {"Versions": {"Base Plan": "TM1py Test Version"}}
      exception: TypeError

test_tm1_cube_object_metadata_collect_cube_dimensions_not_empty:
  -
    cube_name: "Group Employee"

test_tm1_cube_object_metadata_collect_cube_dimensions_match_dimensions:
  -
    cube_name: "Group Employee"
    expected_dimensions: ['Groups', 'Employees', 'Measures Group Employee']

test_tm1_cube_object_metadata_collect_filter_dict_not_empty:
  -
    data_mdx:
      "SELECT 
           {[Periods].[Periods].[202301],[Periods].[Periods].[202302],[Periods].[Periods].[202303],
           [Periods].[Periods].[202304],[Periods].[Periods].[202305],[Periods].[Periods].[202306],
           [Periods].[Periods].[202307],[Periods].[Periods].[202308],[Periods].[Periods].[202309],
           [Periods].[Periods].[202310],[Periods].[Periods].[202311],[Periods].[Periods].[202312]}  
          ON COLUMNS , 
           {[Groups].[Groups].Members}
           * {[Employees].[Employees].Members} 
          ON ROWS 
        FROM [Cost and FTE by Groups] 
        WHERE 
          (
           [Versions].[Versions].[Base Plan], 
           [Lineitems Cost and FTE by Groups].[Lineitems Cost and FTE by Groups].[FTE],
           [Measures Cost and FTE by Groups].[Measures Cost and FTE by Groups].[Value]
          )"

test_tm1_cube_object_metadata_collect_filter_dict_match:
  -
    data_mdx:
        "SELECT 
             {[Periods].[Periods].[202301],[Periods].[Periods].[202302],[Periods].[Periods].[202303],
             [Periods].[Periods].[202304],[Periods].[Periods].[202305],[Periods].[Periods].[202306],
             [Periods].[Periods].[202307],[Periods].[Periods].[202308],[Periods].[Periods].[202309],
             [Periods].[Periods].[202310],[Periods].[Periods].[202311],[Periods].[Periods].[202312]}  
            ON COLUMNS , 
             {[Groups].[Groups].Members}
             * {[Employees].[Employees].Members} 
            ON ROWS 
          FROM [Cost and FTE by Groups] 
          WHERE 
            (
             [Versions].[Versions].[Base Plan], 
             [Lineitems Cost and FTE by Groups].[Lineitems Cost and FTE by Groups].[FTE],
             [Measures Cost and FTE by Groups].[Measures Cost and FTE by Groups].[Value]
            )"
    expected_filter_dict: {
      "Versions":"Base Plan",
      "Lineitems Cost and FTE by Groups":"FTE",
      "Measures Cost and FTE by Groups":"Value"
    }

# Main: MDX query to normalized pandas dataframe functions
example_data_mdx:
  -
    data_mdx:
      "SELECT 
           {[Periods].[Periods].[202301],[Periods].[Periods].[202302],[Periods].[Periods].[202303],
           [Periods].[Periods].[202304],[Periods].[Periods].[202305],[Periods].[Periods].[202306],
           [Periods].[Periods].[202307],[Periods].[Periods].[202308],[Periods].[Periods].[202309],
           [Periods].[Periods].[202310],[Periods].[Periods].[202311],[Periods].[Periods].[202312]}  
          ON COLUMNS , 
           {[Groups].[Groups].Members}
           * {[Employees].[Employees].Members} 
          ON ROWS 
        FROM [Cost and FTE by Groups] 
        WHERE 
          (
           [Versions].[Versions].[Base Plan], 
           [Lineitems Cost and FTE by Groups].[Lineitems Cost and FTE by Groups].[FTE],
           [Measures Cost and FTE by Groups].[Measures Cost and FTE by Groups].[Value]
          )"

test_build_mdx_from_cube_filter_is_valid_format_true:
  -
    cube_filter: {"Versions": {"Base Plan": "TM1py Test Version"}}
    cube_name: "Cost and FTE by Groups"
    expected_mdx:
      "SELECT\r\n
         {[versions].[versions].[baseplan]} DIMENSION PROPERTIES MEMBER_NAME ON 0,\r\n
         {TM1FILTERBYLEVEL({TM1SUBSETALL([groups].[groups])},0)} *
         {TM1FILTERBYLEVEL({TM1SUBSETALL([periods].[periods])},0)} *
         {TM1FILTERBYLEVEL({TM1SUBSETALL([employees].[employees])},0)} *
         {TM1FILTERBYLEVEL({TM1SUBSETALL([lineitemscostandftebygroups].[lineitemscostandftebygroups])},0)}
         *
         {TM1FILTERBYLEVEL({TM1SUBSETALL([measurescostandftebygroups].[measurescostandftebygroups])},0)}
         DIMENSION PROPERTIES MEMBER_NAME ON 1\r\n
         FROM [costandftebygroups]"

test_mdx_to_dataframe_execute_query_success:
  -
    data_mdx:
      "SELECT 
           {[Periods].[Periods].[202301],[Periods].[Periods].[202302],[Periods].[Periods].[202303],
           [Periods].[Periods].[202304],[Periods].[Periods].[202305],[Periods].[Periods].[202306],
           [Periods].[Periods].[202307],[Periods].[Periods].[202308],[Periods].[Periods].[202309],
           [Periods].[Periods].[202310],[Periods].[Periods].[202311],[Periods].[Periods].[202312]}  
          ON COLUMNS , 
           {[Groups].[Groups].Members}
           * {[Employees].[Employees].Members} 
          ON ROWS 
        FROM [Cost and FTE by Groups] 
        WHERE 
          (
           [Versions].[Versions].[Base Plan], 
           [Lineitems Cost and FTE by Groups].[Lineitems Cost and FTE by Groups].[FTE],
           [Measures Cost and FTE by Groups].[Measures Cost and FTE by Groups].[Value]
          )"
  -
    data_mdx:
     "SELECT\r\n
         {[versions].[versions].[baseplan]} DIMENSION PROPERTIES MEMBER_NAME ON 0,\r\n
         {TM1FILTERBYLEVEL({TM1SUBSETALL([groups].[groups])},0)} *
         {TM1FILTERBYLEVEL({TM1SUBSETALL([periods].[periods])},0)} *
         {TM1FILTERBYLEVEL({TM1SUBSETALL([employees].[employees])},0)} *
         {TM1FILTERBYLEVEL({TM1SUBSETALL([lineitemscostandftebygroups].[lineitemscostandftebygroups])},0)}
         *
         {TM1FILTERBYLEVEL({TM1SUBSETALL([measurescostandftebygroups].[measurescostandftebygroups])},0)}
         DIMENSION PROPERTIES MEMBER_NAME ON 1\r\n
         FROM [costandftebygroups]"

test_mdx_to_dataframe_execute_query_fail:
  -
    data_mdx:
      "    {[Periods].[Periods].[202301],[Periods].[Periods].[202302],[Periods].[Periods].[202303],
           [Periods].[Periods].[202304],[Periods].[Periods].[202305],[Periods].[Periods].[202306],
           [Periods].[Periods].[202307],[Periods].[Periods].[202308],[Periods].[Periods].[202309],
           [Periods].[Periods].[202310],[Periods].[Periods].[202311],[Periods].[Periods].[202312]}  
          ON COLUMNS , 
           {[Groups].[Groups].Members}
           * {[Employees].[Employees].Members} 
          ON ROWS 
        FROM [Cost and FTE by Groups] 
        WHERE 
          (
           [Versions].[Versions].[Base Plan], 
           [Lineitems Cost and FTE by Groups].[Lineitems Cost and FTE by Groups].[FTE],
           [Measures Cost and FTE by Groups].[Measures Cost and FTE by Groups].[Value]
          )"
  -
    data_mdx: ""

test_normalize_dataframe_is_dataframe_true:
  -
    data_mdx:
      "SELECT 
           {[Periods].[Periods].[202301],[Periods].[Periods].[202302],[Periods].[Periods].[202303],
           [Periods].[Periods].[202304],[Periods].[Periods].[202305],[Periods].[Periods].[202306],
           [Periods].[Periods].[202307],[Periods].[Periods].[202308],[Periods].[Periods].[202309],
           [Periods].[Periods].[202310],[Periods].[Periods].[202311],[Periods].[Periods].[202312]}  
          ON COLUMNS , 
           {[Groups].[Groups].Members}
           * {[Employees].[Employees].Members} 
          ON ROWS 
        FROM [Cost and FTE by Groups] 
        WHERE 
          (
           [Versions].[Versions].[Base Plan], 
           [Lineitems Cost and FTE by Groups].[Lineitems Cost and FTE by Groups].[FTE],
           [Measures Cost and FTE by Groups].[Measures Cost and FTE by Groups].[Value]
          )"

test_normalize_dataframe_match_number_of_dimensions_success:
  -
    data_mdx:
      "SELECT 
           {[Periods].[Periods].[202301],[Periods].[Periods].[202302],[Periods].[Periods].[202303],
           [Periods].[Periods].[202304],[Periods].[Periods].[202305],[Periods].[Periods].[202306],
           [Periods].[Periods].[202307],[Periods].[Periods].[202308],[Periods].[Periods].[202309],
           [Periods].[Periods].[202310],[Periods].[Periods].[202311],[Periods].[Periods].[202312]}  
          ON COLUMNS , 
           {[Groups].[Groups].Members}
           * {[Employees].[Employees].Members} 
          ON ROWS 
        FROM [Cost and FTE by Groups] 
        WHERE 
          (
           [Versions].[Versions].[Base Plan], 
           [Lineitems Cost and FTE by Groups].[Lineitems Cost and FTE by Groups].[FTE],
           [Measures Cost and FTE by Groups].[Measures Cost and FTE by Groups].[Value]
          )"
    expected_dimensions: 7

test_normalize_dataframe_match_dimensions_success:
  -
    data_mdx:
      "SELECT 
           {[Periods].[Periods].[202301],[Periods].[Periods].[202302],[Periods].[Periods].[202303],
           [Periods].[Periods].[202304],[Periods].[Periods].[202305],[Periods].[Periods].[202306],
           [Periods].[Periods].[202307],[Periods].[Periods].[202308],[Periods].[Periods].[202309],
           [Periods].[Periods].[202310],[Periods].[Periods].[202311],[Periods].[Periods].[202312]}  
          ON COLUMNS , 
           {[Groups].[Groups].Members}
           * {[Employees].[Employees].Members} 
          ON ROWS 
        FROM [Cost and FTE by Groups] 
        WHERE 
          (
           [Versions].[Versions].[Base Plan], 
           [Lineitems Cost and FTE by Groups].[Lineitems Cost and FTE by Groups].[FTE],
           [Measures Cost and FTE by Groups].[Measures Cost and FTE by Groups].[Value]
          )"
    expected_dimensions: ['Groups', 'Versions', 'Periods', 'Employees', 'Lineitems Cost and FTE by Groups', 'Measures Cost and FTE by Groups', 'Value']

test_build_mdx_from_cube_filter_create_dataframe_success:
  -
    cube_filter: {"Versions": {"Base Plan": "TM1py Test Version"}}
    cube_name: "Cost and FTE by Groups"


test_dataframe_filter_inplace:
  -
    dataframe: {
      "Versions": [ "Base Plan", "Plan1", "Plan2" ],
      "Orgunit": [ "ASD123", "ASD123", "XYZ123" ],
      "Value": [ 1, 0, 1 ],
    }
    filter_condition: { "Orgunit": "ASD123" }
    expected_dataframe: {
      "Versions": [ "Base Plan", "Plan1" ],
      "Orgunit": [ "ASD123", "ASD123" ],
      "Value": [ 1, 0 ],
    }

test_dataframe_filter:
  -
    dataframe: {
      "Versions": [ "Base Plan", "Plan1", "Plan2" ],
      "Orgunit": [ "ASD123", "ASD123", "XYZ123" ],
      "Value": [ 1, 0, 1 ],
    }
    filter_condition: { "Orgunit": "ASD123" }
    expected_dataframe: {
      "Versions": [ "Base Plan", "Plan1" ],
      "Orgunit": [ "ASD123", "ASD123" ],
      "Value": [ 1, 0 ],
    }

test_dataframe_drop_column:
  -
    dataframe: {
      "Versions": [ "Base Plan", "Plan1", "Plan2" ],
      "Orgunit": [ "ASD123", "ASD123", "XYZ123" ],
      "Value": [ 1, 0, 1 ]
    }
    column_list: ["Orgunit"]
    expected_dataframe: {
      "Versions": [ "Base Plan", "Plan1", "Plan2" ],
      "Value": [ 1, 0, 1 ]
    }

test_dataframe_redimension_scale_down:
  -
    dataframe: {
      "Versions": [ "Base Plan", "Plan1", "Plan2" ],
      "Orgunit": [ "ASD123", "ASD123", "XYZ123" ],
      "Value": [ 1, 0, 1 ]
    }
    filter_condition: { "Orgunit": "ASD123" }
    expected_dataframe: {
      "Versions": [ "Base Plan", "Plan1" ],
      "Value": [ 1, 0]
    }
  -
    dataframe: {
    "Versions": [ "Base Plan", "Plan1", "Plan2" ],
    "Orgunit": [ "ASD123", "ASD123", "XYZ123" ],
    "Value": [ 1, 0, 1 ]
  }
    filter_condition: { "Orgunit": "XYZ123" }
    expected_dataframe: {
      "Versions": [ "Plan2" ],
      "Value": [ 1 ]
    }

test_dataframe_relabel:
  -
    dataframe: {
      "Versions": [ "Base Plan", "Plan1", "Plan2" ],
      "Orgunit": [ "ASD123", "ASD123", "XYZ123" ],
      "Value": [ 1, 0, 1 ]
    }
    columns: { "Orgunit": "Orgunit2" }
    expected_columns: ["Versions", "Orgunit2", "Value"]
  -
    dataframe: {
    "Versions": [ "Base Plan", "Plan1", "Plan2" ],
    "Orgunit": [ "ASD123", "ASD123", "XYZ123" ],
    "Value": [ 1, 0, 1 ]
  }
    columns: { "Orgunit": "Orgunit2", "Versions": "Base Version" }
    expected_columns: [ "Base Version", "Orgunit2", "Value" ]


test_dataframe_add_column_assign_value:
  -
    dataframe: {
      "Versions": [ "Base Plan", "Plan1", "Plan2" ],
      "Orgunit": [ "ASD123", "ASD123", "XYZ123" ],
    }
    column_values: { "Employee": "Alice" }
    expected_dataframe: {
      "Versions": [ "Base Plan", "Plan1", "Plan2" ],
      "Orgunit": [ "ASD123", "ASD123", "XYZ123" ],
      "Employee": ["Alice", "Alice", "Alice"],
    }

test_dataframe_redimension_and_transform:
  -
    dataframe: {
      "Versions": [ "Base Plan", "Plan1", "Plan2" ],
      "Period": ["202401", "202402", "202402"],
      "Orgunit": [ "ASD123", "ASD123", "XYZ123" ],
    }
    source_dim_mapping: { "Orgunit": "ASD123" }
    related_dimensions: { "Versions": "Base Versions" }
    target_dim_mapping: { "Employee": "Alice" }
    expected_dataframe: {
      "Base Versions": [ "Base Plan", "Plan1" ],
      "Period": [ "202401", "202402" ],
      "Employee": ["Alice", "Alice"],
    }
  -
    dataframe: {
    "Versions": [ "Base Plan", "Plan1", "Plan2" ],
    "Period": [ "202401", "202402", "202402" ],
    "Orgunit": [ "ASD123", "ASD123", "XYZ123" ],
  }
    source_dim_mapping: { "Orgunit": "ASD123", "Period": "202402"}
    related_dimensions: { "Versions": "Base Versions" }
    target_dim_mapping: { "Employee": "Alice" }
    expected_dataframe: {
      "Base Versions": [ "Plan1" ],
      "Employee": [ "Alice" ],
    }

test_dataframe_reorder_dimensions:
  -
    dataframe: {
      "Period": [ "202202", "202202", "202202", "202202" ],
      "Employee": [ "00000001", "00000002", "00000003", "00000004" ],
      "OrgUnit": [ "ABC123A", "ABC123A", "XYZ123X", "XYZ123X" ],
      "Measure": [ "Value", "Value", "Value", "Value" ],
      "Value": [ 120, 120, 120, 120 ]
    }
    cube_cols: ["Period", "OrgUnit", "Employee", "Measure"]
    expected_dataframe: {
      "Period": [ "202202", "202202", "202202", "202202" ],
      "OrgUnit": [ "ABC123A", "ABC123A", "XYZ123X", "XYZ123X" ],
      "Employee": [ "00000001", "00000002", "00000003", "00000004" ],
      "Measure": [ "Value", "Value", "Value", "Value" ],
      "Value": [ 120, 120, 120, 120 ]
    }

test_dataframe_value_scale:
  -
    dataframe: {
      "Period": [ "202202", "202202", "202202", "202202" ],
      "Value": [ 120, 120, 120, 120 ]
    }
    # after add one, multiply by two
    expected_dataframe: {
      "Period": [ "202202", "202202", "202202", "202202" ],
      "Value": [ 242, 242, 242, 242 ]
    }

test_dataframe_itemskip_elements:
  -
    source: {
      "A": [ "apple", "banana", "cherry", "date" ],
      "B": [ "x", "y", "z", "w" ],
      "Value": [ 10, 20, 30, 40 ]
    }
    check1: {"A": ["apple", "cherry"]}
    check2: {"B": ["x", "y"]}
    defaults: {}
    expected: {
      "A": [ "apple" ],
      "B": [ "x" ],
      "Value": [ 10 ]
    }
  -
    source: {
      "A": [ "apple", "banana", "cherry", "date" ],
      "B": [ "x", "y", "z", "w" ],
      "Value": [ 10, 20, 30, 40 ]
    }
    check1: { "A": [ "apple", "cherry" ] }
    check2: { "B": [ "x", "y" ] }
    defaults: {"A": "default_elem"}
    expected: {
      "A": [ "apple", "default_elem" ],
      "B": [ "x", "y" ],
      "Value": [ 10, 20 ]
    }

test_dataframe_validate_datatypes:
  -
    measuredim: "Measures"
    measuretypes: {
        "Numeric": "Numeric",
        "String": "String"
    }
    dataframe: {
        "Lineitems": ["Test1", "Test2", "Test3", "Test4"],
        "Measures": ["Numeric", "String", "Numeric", "String"],
        "Value": ["1.23", "ABCD", "-5.555555", "EFGH"]
    }
    expected: {
      "Lineitems": [ "Test1", "Test2", "Test3", "Test4" ],
      "Measures": [ "Numeric", "String", "Numeric", "String" ],
      "Value": [ 1.23, "ABCD", -5.555555, "EFGH" ]
    }
  -
    measuredim: "Measures"
    measuretypes: {
      "Numeric": "Numeric",
      "String": "String"
    }
    dataframe: {
      "Lineitems": [ "Test1", "Test2", "Test3", "Test4" ],
      "Measures": [ "Numeric", "String", "Numeric", "String" ],
      "Value": [ "1,23", "ABCD", "-5,555555", "EFGH" ]
    }
    expected: {
      "Lineitems": [ "Test1", "Test2", "Test3", "Test4" ],
      "Measures": [ "Numeric", "String", "Numeric", "String" ],
      "Value": [ 1.23, "ABCD", -5.555555, "EFGH" ]
    }
  -
    measuredim: "Measures"
    measuretypes: {
      "Numeric": "Numeric",
      "String": "String"
    }
    dataframe: {
      "Lineitems": [ "Test1", "Test2", "Test3", "Test4" ],
      "Measures": [ "Numeric", "String", "Numeric", "String" ],
      "Value": [ "1.23", "ABCD", "-5,555555", "EFGH" ]
    }
    expected: {
      "Lineitems": [ "Test1", "Test2", "Test3", "Test4" ],
      "Measures": [ "Numeric", "String", "Numeric", "String" ],
      "Value": [ 1.23, "ABCD", -5.555555, "EFGH" ]
    }
  -
    measuredim: "Measures"
    measuretypes: {
      "Numeric": "Numeric",
      "String": "String"
    }
    dataframe: {
      "Lineitems": [ "Test1", "Test2", "Test3", "Test4" ],
      "Measures": [ "Numeric", "String", "Numeric", "String" ],
      "Value": [ "5.e-003", "ABCD", "-4,e-003", "EFGH" ]
    }
    expected: {
      "Lineitems": [ "Test1", "Test2", "Test3", "Test4" ],
      "Measures": [ "Numeric", "String", "Numeric", "String" ],
      "Value": [ 0.005, "ABCD", -0.004, "EFGH" ]
    }

# Main: tests for dataframe remapping and copy functions
test_dataframe_find_and_replace_success:
  -
    dataframe: { "Versions": ["Base Plan", "Plan1"]}
    mapping: { "Versions": {"Base Plan":"TM1py Test Version"} }
    expected_dataframe: { "Versions": [ "TM1py Test Version", "Plan1" ] }
  -
    dataframe: { "Versions": ["Base Plan", "Plan1"]}
    mapping: { "Versions": {"Base Plan":"TM1py Test Version", "Plan1": "Plan2"} }
    expected_dataframe: { "Versions": [ "TM1py Test Version", "Plan2" ] }

test_dataframe_find_and_replace_fail:
  -
    dataframe: { "Versions": ["Base Plan"]}
    mapping: {}
    expected_dataframe: { "Versions": [ "TM1py Test Version" ] }
  -
    dataframe: { "Versions": ["Base Plan"]}
    mapping: { "Versions": {} }
    expected_dataframe: { "Versions": [ "TM1py Test Version" ] }
  -
    dataframe: { "Versions": ["Plan"]}
    mapping: { "Versions": {"Base Plan":"TM1py Test Version"} }
    expected_dataframe: { "Versions": [ "TM1py Test Version" ] }
  -
    dataframe: { "Versions": []}
    mapping: { "Versions": {"Base Plan":"TM1py Test Version"} }
    expected_dataframe: { "Versions": [ "TM1py Test Version" ] }
  -
    dataframe: {}
    mapping: { "Versions": {"Base Plan":"TM1py Test Version"} }
    expected_dataframe: { "Versions": [ "TM1py Test Version" ] }

test_dataframe_replace_success:
  -
    dataframe: {
      "period": ["202202", "202203", "202202", "202203"],
      "dim_to_map": ["source1", "source2", "other", "other"],
      "dim2_to_map": ["source3", "other", "source3", "other"],
      "measure": ["value", "value", "value", "value"],
      "value": [100, 100, 100, 100]
    }
    mapping: {
      "dim_to_map": { "source1": "target1", "source2": "target2" },
      "dim2_to_map": { "source3": "target3" }
    }
    expected_dataframe: {
      "period": ["202202", "202203", "202202", "202203"],
      "dim_to_map": ["target1", "target2", "other", "other"],
      "dim2_to_map": ["target3", "other", "target3", "other"],
      "measure": ["value", "value", "value", "value"],
      "value": [100, 100, 100, 100]
    }

test_dataframe_map_and_replace_success:
  -
    dataframe: {
      "Period": ["202202", "202202", "202202", "202202"],
      "Employee": ["00000001", "00000002", "00000003", "00000004"],
      "OrgUnit": ["ABC123A", "ABC123A", "XYZ123X", "XYZ123X"],
      "Measure": ["Value", "Value", "Value", "Value"],
      "Value": [120, 120, 120, 120]
    }
    mapping_dataframe: {
      "Period": ["202202", "202202", "202202", "202202"],
      "Employee": ["00000001", "00000002", "00000003", "00000004"],
      "Employee Settings": ["ORG_UNIT_PARENT", "ORG_UNIT_PARENT", "ORG_UNIT_PARENT", "ORG_UNIT_PARENT"],
      "Value": ["AAA0001", "AAA0002", "AAA0001", "AAA0002"]
    }
    mapping_dimensions: {
      "OrgUnit": "Value"
    }
    expected_dataframe: {
      "Period": ["202202", "202202", "202202", "202202"],
      "Employee": ["00000001", "00000002", "00000003", "00000004"],
      "OrgUnit": ["AAA0001", "AAA0002", "AAA0001", "AAA0002"],
      "Measure": ["Value", "Value", "Value", "Value"],
      "Value": [120, 120, 120, 120]
    }
  -
    dataframe: {
      "Period": [ "202202", "202202", "202202", "202202" ],
      "Employee": [ "00000001", "00000002", "00000003", "00000004" ],
      "OrgUnit": [ "ABC123A", "ABC123A", "XYZ123X", "XYZ123X" ],
      "Measure": [ "Value", "Value", "Value", "Value" ],
      "Value": [ 120, 120, 120, 120 ]
    }
    mapping_dataframe: {
      "Period": [ "202202", "202202", "202202", "202202" ],
      "Employee": [ "00000001", "00000002", "00000003", "00000004" ],
      "OrgUnit": ["AAA0001", "AAA0002", "AAA0001", "AAA0002"],
      "Value": [1, 1, 1, 1]
    }
    mapping_dimensions: {
      "OrgUnit": "OrgUnit"
    }
    expected_dataframe: {
      "Period": [ "202202", "202202", "202202", "202202" ],
      "Employee": [ "00000001", "00000002", "00000003", "00000004" ],
      "OrgUnit": [ "AAA0001", "AAA0002", "AAA0001", "AAA0002" ],
      "Measure": [ "Value", "Value", "Value", "Value" ],
      "Value": [ 120, 120, 120, 120 ]
    }

test_dataframe_map_and_join_success:
  -
    dataframe: {
      "Period": ["202201", "202201", "202202", "202202"],
      "Employee": ["00000001", "00000002", "00000001", "00000002"],
      "Orgunit": ["abc123a", "abc123a", "def345a", "def345a"],
      "Measure": ["cost001", "cost001", "cost001", "cost001"],
      "Value": [10, 10, 10, 10]
    }
    joined_cols: ["Sales Channel", "Specialism"]
    mapping_dataframe: {
      "Period": ["202201", "202201", "202202", "202202"],
      "Employee": ["00000001", "00000002", "00000001", "00000002"],
      "Sales Channel": ["temp", "perm", "perm", "perm"],
      "Specialism": ["IT", "finance", "engineering", "engineering"],
      "Cost Position Type": ["KAM", "AM", "Team Leader", "Manager"]
    }
    expected_dataframe: {
      "Period": ["202201", "202201", "202202", "202202"],
      "Employee": ["00000001", "00000002", "00000001", "00000002"],
      "Orgunit": ["abc123a", "abc123a", "def345a", "def345a"],
      "Measure": ["cost001", "cost001", "cost001", "cost001"],
      "Value": [10, 10, 10, 10],
      "Sales Channel": ["temp", "perm", "perm", "perm"],
      "Specialism": ["IT", "finance", "engineering", "engineering"]
    }

test_dataframe_execute_mappings_replace_success:
  -
    dataframe: {
    "Period": [ "202201", "202201", "202202", "202202" ],
    "Employee": [ "00000001", "00000002", "00000001", "00000002" ],
    "Orgunit": [ "abc123a", "abc123a", "def345a", "def345a" ],
    "Measure": [ "cost001", "cost001", "cost001", "cost001" ],
    "Value": [ 10, 10, 10, 10 ]
    }
    mapping_steps: [
      {
        "method": "replace",
        "mapping": {"Orgunit": {"abc123a": "xyz987z"}}
      }
    ]
    expected_dataframe: {
      "Period": [ "202201", "202201", "202202", "202202" ],
      "Employee": [ "00000001", "00000002", "00000001", "00000002" ],
      "Orgunit": [ "xyz987z", "xyz987z", "def345a", "def345a" ],
      "Measure": [ "cost001", "cost001", "cost001", "cost001" ],
      "Value": [ 10, 10, 10, 10 ]
    }

test_dataframe_execute_mappings_map_and_replace_shared_df_with_filter_and_relabel_success:
  -
    mapping_steps: [
      {
        "method": "map_and_replace",
        # since this is a test for shared mapping only, no step-specific mapping df is specified
        "mapping_filter": { "filter_dim": "filter_map_and_replace" },
        "mapping_dimensions": { "map_and_replace_dim_source": "map_and_replace_dim_target" },
        "relabel_dimensions": True
      }
    ]

test_dataframe_execute_mappings_map_and_join_shared_with_filter_and_drop_success:
  -
    mapping_steps: [
      {
        "method": "map_and_join",
        # since this is a test for shared mapping only, no step-specific mapping df is specified
        "mapping_filter": { "filter_dim": "filter_map_and_join" },
        "joined_columns": ["col_to_join1", "col_to_join2"],
        "dropped_columns": ["col_to_drop1", "col_to_drop2"]

      }
    ]

test_mssql_extract_full_table:
  -
    table_name: "Unit Test Table"
    expected: {
      "UnitTestStringColumn1": [ 'TestString1_1', 'TestString2_1' ],
      "UnitTestStringColumn2": [ 'TestString1_2', 'TestString2_2' ],
      "UnitTestFloatColumn1": [ 12.34, 12.34 ],
      "UnitTestIntColumn2": [ 123, 123 ]
    }

test_mssql_extract_table_columns:
  -
    table_name: "Unit Test Table"
    columns: ["UnitTestStringColumn1", "UnitTestFloatColumn1"]
    expected: {
      "UnitTestStringColumn1": [ 'TestString1_1', 'TestString2_1' ],
      "UnitTestFloatColumn1": [ 12.34, 12.34 ],
    }

test_mssql_extract_query:
  -
    query: "SELECT [UnitTestStringColumn1], [UnitTestFloatColumn1] FROM dbo.[Unit Test Table]"
    expected: {
      "UnitTestStringColumn1": [ 'TestString1_1', 'TestString2_1' ],
      "UnitTestFloatColumn1": [ 12.34, 12.34 ],
    }
  -
    query: "SELECT * FROM dbo.[Unit Test Table]"
    expected: {
      "UnitTestStringColumn1": [ 'TestString1_1', 'TestString2_1' ],
      "UnitTestStringColumn2": [ 'TestString1_2', 'TestString2_2' ],
      "UnitTestFloatColumn1": [ 12.34, 12.34 ],
      "UnitTestIntColumn2": [ 123, 123 ]
    }

test_mssql_extract_query_with_chunksize:
  -
    query: "SELECT [UnitTestStringColumn1], [UnitTestFloatColumn1] FROM dbo.[Unit Test Table]"
    expected: {
      "UnitTestStringColumn1": [ 'TestString1_1', 'TestString2_1' ],
      "UnitTestFloatColumn1": [ 12.34, 12.34 ],
    }
    chunksize: 1
  -
    query: "SELECT * FROM dbo.[Unit Test Table]"
    expected: {
      "UnitTestStringColumn1": [ 'TestString1_1', 'TestString2_1' ],
      "UnitTestStringColumn2": [ 'TestString1_2', 'TestString2_2' ],
      "UnitTestFloatColumn1": [ 12.34, 12.34 ],
      "UnitTestIntColumn2": [ 123, 123 ]
    }
    chunksize: 2

test_sql_normalize_relabel:
  -
    dataframe: {
      "UnitTestStringColumnX": [ 'TestString1_1', 'TestString2_1' ],
      "UnitTestStringColumnY": [ 'TestString1_2', 'TestString2_2' ]
    }
    column_mapping: {"UnitTestStringColumnX":"UnitTestStringColumn1",
                     "UnitTestStringColumnY":"UnitTestStringColumn2"}
    expected: {
      "UnitTestStringColumn1": [ 'TestString1_1', 'TestString2_1' ],
      "UnitTestStringColumn2": [ 'TestString1_2', 'TestString2_2' ],
    }

test_sql_normalize_valuecol_assign:
  -
    dataframe: {
      "UnitTestStringColumn1": [ 'TestString1_1', 'TestString2_1' ],
      "UnitTestStringColumn2": [ 'TestString1_2', 'TestString2_2' ],
      "UnitTestFloatColumn1": [ 12.34, 12.34 ],
      "UnitTestIntColumn2": [ 123, 123 ]
    }
    valuecol: "UnitTestFloatColumn1"
    expected: {
      "UnitTestStringColumn1": [ 'TestString1_1', 'TestString2_1' ],
      "UnitTestStringColumn2": [ 'TestString1_2', 'TestString2_2' ],
      "Value": [ 12.34, 12.34 ],
      "UnitTestIntColumn2": [ 123, 123 ]
    }

test_sql_normalize_drop:
  -
    dataframe: {
      "UnitTestStringColumn1": [ 'TestString1_1', 'TestString2_1' ],
      "UnitTestStringColumn2": [ 'TestString1_2', 'TestString2_2' ],
      "UnitTestFloatColumn1": [ 12.34, 12.34 ],
      "UnitTestIntColumn2": [ 123, 123 ]
    }
    drop: ["UnitTestFloatColumn1", "UnitTestIntColumn2"]
    expected: {
      "UnitTestStringColumn1": [ 'TestString1_1', 'TestString2_1' ],
      "UnitTestStringColumn2": [ 'TestString1_2', 'TestString2_2' ],
    }

test_mssql_loader_replace:
  -
    dataframe: {
      "VarcharColumn1X": ["WriteString1_1", "WriteString1_2"],
      "VarcharColumn2X": ["WriteString2_1", "WriteString2_2"],
      "FloatColumn1": [12, 56],
      "Test": [9.331, 111.666]
    }
    table_name: "Write Test Table"
    if_exists: "append"

test_dataframe_casting_for_csv_file:
  -
    dataframe: {
        "Versions": ["Base Plan", "Plan1", "Plan2"],
        "Orgunit": ["ASD123", "ASD123", "XYZ123"],
        "Value": [1, 0, 1],
    }
    cube_dims: ["Versions", "Orgunit"]
  -
    dataframe: {}
    cube_dims: ["Versions", "Orgunit"]
  -
    dataframe: {
        "Versions": ["Base Plan", "Plan1", "Plan2"],
        "Orgunit": [123, 987, 42],
        "Value": [1, 0, 1],
    }
    cube_dims: ["Versions", "Orgunit"]

test_dataframe_to_csv:
  -
    data_dataframe: {
      "ID": ["A001", "A002", "A003", "A004", "A005"],
      "Name": ["Product A", "Product B", "Product C", "Product D", "Product E"],
      "Category": ["Electronics", "Furniture", "Electronics", "Clothing", "Furniture"],
      "Quantity": [10, 5, 8, 12, 7],
      "Price": [199.99, 79.99, 149.99, 39.99, 99.99],
      "Date": ["2024-02-01", "2024-02-02", "2024-02-03", "2024-02-04", "2024-02-05"],
    }
    expected_dataframe: {
      "ID": ["A001", "A002", "A003", "A004", "A005"],
      "Name": ["Product A", "Product B", "Product C", "Product D", "Product E"],
      "Category": ["Electronics", "Furniture", "Electronics", "Clothing", "Furniture"],
      "Quantity": [10, 5, 8, 12, 7],
      "Price": [199.99, 79.99, 149.99, 39.99, 99.99],
      "Date": ["2024-02-01", "2024-02-02", "2024-02-03", "2024-02-04", "2024-02-05"],
    }
  -
    data_dataframe: {
      "ID": ["A001", "A002", "A003", "A004", "A005"],
      "Price": [199.99, nan, 149.99, 39.99, nan],
      "Quantity": [10.0, 5.0, NaN, 12.0, 7.0]
    }
    expected_dataframe: {
      "ID": [ "A001", "A002", "A003", "A004", "A005" ],
      "Price": [ 199.99, nan, 149.99, 39.99, nan ],
      "Quantity": [ 10.0, 5.0, NaN, 12.0, 7.0 ]
    }

test_dataframe_to_csv_build_dataframe_form_mdx:
  -
    data_mdx:
      "SELECT 
           {[Periods].[Periods].[202301],[Periods].[Periods].[202302],[Periods].[Periods].[202303],
           [Periods].[Periods].[202304],[Periods].[Periods].[202305],[Periods].[Periods].[202306],
           [Periods].[Periods].[202307],[Periods].[Periods].[202308],[Periods].[Periods].[202309],
           [Periods].[Periods].[202310],[Periods].[Periods].[202311],[Periods].[Periods].[202312]}  
          ON COLUMNS , 
           {[Groups].[Groups].Members}
           * {[Employees].[Employees].Members} 
          ON ROWS 
        FROM [Cost and FTE by Groups] 
        WHERE 
          (
           [Versions].[Versions].[Base Plan], 
           [Lineitems Cost and FTE by Groups].[Lineitems Cost and FTE by Groups].[FTE],
           [Measures Cost and FTE by Groups].[Measures Cost and FTE by Groups].[Value]
          )"

test_dataframe_to_csv_build_dataframe_form_mdx_with_param_optimisation:
  -
    data_mdx:
      "SELECT 
           {[Periods].[Periods].[202301],[Periods].[Periods].[202302],[Periods].[Periods].[202303],
           [Periods].[Periods].[202304],[Periods].[Periods].[202305],[Periods].[Periods].[202306],
           [Periods].[Periods].[202307],[Periods].[Periods].[202308],[Periods].[Periods].[202309],
           [Periods].[Periods].[202310],[Periods].[Periods].[202311],[Periods].[Periods].[202312]}  
          ON COLUMNS , 
           {[Groups].[Groups].Members}
           * {[Employees].[Employees].Members} 
          ON ROWS 
        FROM [Cost and FTE by Groups] 
        WHERE 
          (
           [Versions].[Versions].[Base Plan], 
           [Lineitems Cost and FTE by Groups].[Lineitems Cost and FTE by Groups].[FTE],
           [Measures Cost and FTE by Groups].[Measures Cost and FTE by Groups].[Value]
          )"

test_dataframe_to_csv_build_dataframe_form_mdx_fail:
  -
    data_mdx:
      "SELECT 
           {[Periods].[Periods].[202301],[Periods].[Periods].[202302],[Periods].[Periods].[202303],
           [Periods].[Periods].[202304],[Periods].[Periods].[202305],[Periods].[Periods].[202306],
           [Periods].[Periods].[202307],[Periods].[Periods].[202308],[Periods].[Periods].[202309],
           [Periods].[Periods].[202310],[Periods].[Periods].[202311],[Periods].[Periods].[202312]}  
          ON COLUMNS , 
           {[Groups].[Groups].Members}
           * {[Employees].[Employees].Members} 
          ON ROWS 
        FROM [Cost and FTE by Groups] 
        WHERE 
          (
           [Versions].[Versions].[Base Plan], 
           [Lineitems Cost and FTE by Groups].[Lineitems Cost and FTE by Groups].[FTE],
           [Measures Cost and FTE by Groups].[Measures Cost and FTE by Groups].[Value]
          )"

test_generate_mapping_queries_for_slice:
  -
    kwargs: { "Periods": "202202", "Versions": "Actual" }
    ms:
      - mapping_mdx_template: |
          SELECT
          {[Periods].[Periods].[$Periods]}
          ON COLUMNS,
          {[Versions].[Versions].[$Versions]}
          FROM [CubeName]
    sm: null
    expected_ms:
      - mapping_mdx_template: |
          SELECT
          {[Periods].[Periods].[$Periods]}
          ON COLUMNS,
          {[Versions].[Versions].[$Versions]}
          FROM [CubeName]
        mapping_mdx: |
          SELECT
          {[Periods].[Periods].[202202]}
          ON COLUMNS,
          {[Versions].[Versions].[Actual]}
          FROM [CubeName]
    expected_sm: null
  -
    kwargs: { "Periods": "202202", "Versions": "Actual" }
    ms:
      - mapping_sql_template: |
          select
              [Versions],
              [Periods],
              [Key Account Managers],
              [Business Partners],
              [Measures Key Account Manager to Business Partner],
              [Value]
          from [PlanData].[dbo].[IFL_FACT_RESTATED_KAM_TO_BP]
          where [Periods] = $Periods and [Versions] = '$Versions'
          and [Key Account Managers] <> 'legacy k' and [Key Account Managers] is not null
          and [Business Partners] is not null
    sm: null
    expected_ms:
      - mapping_sql_template: |
          select
              [Versions],
              [Periods],
              [Key Account Managers],
              [Business Partners],
              [Measures Key Account Manager to Business Partner],
              [Value]
          from [PlanData].[dbo].[IFL_FACT_RESTATED_KAM_TO_BP]
          where [Periods] = $Periods and [Versions] = '$Versions'
          and [Key Account Managers] <> 'legacy k' and [Key Account Managers] is not null
          and [Business Partners] is not null
        mapping_sql_query: |
          select
              [Versions],
              [Periods],
              [Key Account Managers],
              [Business Partners],
              [Measures Key Account Manager to Business Partner],
              [Value]
          from [PlanData].[dbo].[IFL_FACT_RESTATED_KAM_TO_BP]
          where [Periods] = 202202 and [Versions] = 'Actual'
          and [Key Account Managers] <> 'legacy k' and [Key Account Managers] is not null
          and [Business Partners] is not null
    expected_sm: null
  -
    kwargs: {"Periods": "202202", "Versions": "Actual"}
    ms:
      -
        mapping_mdx_template: |
          SELECT
          {[Periods].[Periods].[$Periods]}
          ON COLUMNS,
          {[Versions].[Versions].[$Versions]}
          FROM [CubeName]
      -
        mapping_sql_template: |
          select
              [Versions],
              [Periods],
              [Key Account Managers],
              [Business Partners],
              [Measures Key Account Manager to Business Partner],
              [Value]
          from [PlanData].[dbo].[IFL_FACT_RESTATED_KAM_TO_BP]
          where [Periods] = $Periods and [Versions] = '$Versions'
          and [Key Account Managers] <> 'legacy k' and [Key Account Managers] is not null
          and [Business Partners] is not null
    sm: null
    expected_ms:
      -
        mapping_mdx_template: |
          SELECT
          {[Periods].[Periods].[$Periods]}
          ON COLUMNS,
          {[Versions].[Versions].[$Versions]}
          FROM [CubeName]
        mapping_mdx: |
          SELECT
          {[Periods].[Periods].[202202]}
          ON COLUMNS,
          {[Versions].[Versions].[Actual]}
          FROM [CubeName]
      -
        mapping_sql_template: |
          select
              [Versions],
              [Periods],
              [Key Account Managers],
              [Business Partners],
              [Measures Key Account Manager to Business Partner],
              [Value]
          from [PlanData].[dbo].[IFL_FACT_RESTATED_KAM_TO_BP]
          where [Periods] = $Periods and [Versions] = '$Versions'
          and [Key Account Managers] <> 'legacy k' and [Key Account Managers] is not null
          and [Business Partners] is not null
        mapping_sql_query: |
          select
              [Versions],
              [Periods],
              [Key Account Managers],
              [Business Partners],
              [Measures Key Account Manager to Business Partner],
              [Value]
          from [PlanData].[dbo].[IFL_FACT_RESTATED_KAM_TO_BP]
          where [Periods] = 202202 and [Versions] = 'Actual'
          and [Key Account Managers] <> 'legacy k' and [Key Account Managers] is not null
          and [Business Partners] is not null
    expected_sm: null
  -
    kwargs: {"Periods": "202202", "Versions": "Actual"}
    ms: null
    sm: null
    expected_ms: null
    expected_sm: null
  -
    kwargs: { "Periods": "202202", "Versions": "Actual" }
    ms: null
    sm:
      mapping_mdx_template: |
        SELECT
        {[Periods].[Periods].[$Periods]}
        ON COLUMNS,
        {[Versions].[Versions].[$Versions]}
        FROM [CubeName]

    expected_ms: null
    expected_sm:
      mapping_mdx_template: |
        SELECT
        {[Periods].[Periods].[$Periods]}
        ON COLUMNS,
        {[Versions].[Versions].[$Versions]}
        FROM [CubeName]

      mapping_mdx: |
        SELECT
        {[Periods].[Periods].[202202]}
        ON COLUMNS,
        {[Versions].[Versions].[Actual]}
        FROM [CubeName]

  -
    kwargs: { "Periods": "202202", "Versions": "Actual" }
    ms:
      - mapping_mdx_template: |
          SELECT
          {[Periods].[Periods].[$Periods]}
          ON COLUMNS,
          {[Versions].[Versions].[$Versions]}
          FROM [CubeName]

      - mapping_sql_template: |
          select
              [Versions],
              [Periods],
              [Key Account Managers],
              [Business Partners],
              [Measures Key Account Manager to Business Partner],
              [Value]
          from [PlanData].[dbo].[IFL_FACT_RESTATED_KAM_TO_BP]
          where [Periods] = $Periods and [Versions] = '$Versions'
          and [Key Account Managers] <> 'legacy k' and [Key Account Managers] is not null
          and [Business Partners] is not null

    sm:
      mapping_mdx_template: |
        SELECT
        {[Periods].[Periods].[$Periods]}
        ON COLUMNS,
        {[Versions].[Versions].[$Versions]}
        FROM [CubeName]

    expected_ms:
      - mapping_mdx_template: |
          SELECT
          {[Periods].[Periods].[$Periods]}
          ON COLUMNS,
          {[Versions].[Versions].[$Versions]}
          FROM [CubeName]

        mapping_mdx: |
          SELECT
          {[Periods].[Periods].[202202]}
          ON COLUMNS,
          {[Versions].[Versions].[Actual]}
          FROM [CubeName]

      - mapping_sql_template: |
          select
              [Versions],
              [Periods],
              [Key Account Managers],
              [Business Partners],
              [Measures Key Account Manager to Business Partner],
              [Value]
          from [PlanData].[dbo].[IFL_FACT_RESTATED_KAM_TO_BP]
          where [Periods] = $Periods and [Versions] = '$Versions'
          and [Key Account Managers] <> 'legacy k' and [Key Account Managers] is not null
          and [Business Partners] is not null

        mapping_sql_query: |
          select
              [Versions],
              [Periods],
              [Key Account Managers],
              [Business Partners],
              [Measures Key Account Manager to Business Partner],
              [Value]
          from [PlanData].[dbo].[IFL_FACT_RESTATED_KAM_TO_BP]
          where [Periods] = 202202 and [Versions] = 'Actual'
          and [Key Account Managers] <> 'legacy k' and [Key Account Managers] is not null
          and [Business Partners] is not null

    expected_sm:
      mapping_mdx_template: |
        SELECT
        {[Periods].[Periods].[$Periods]}
        ON COLUMNS,
        {[Versions].[Versions].[$Versions]}
        FROM [CubeName]

      mapping_mdx: |
        SELECT
        {[Periods].[Periods].[202202]}
        ON COLUMNS,
        {[Versions].[Versions].[Actual]}
        FROM [CubeName]

  -
    kwargs: { "Periods": "202202", "Versions": "Actual" }
    ms:
      - mapping_sql_template: |
          select
              [Versions],
              [Periods],
              [Key Account Managers],
              [Business Partners],
              [Measures Key Account Manager to Business Partner],
              [Value]
          from [PlanData].[dbo].[IFL_FACT_RESTATED_KAM_TO_BP]
          where [Periods] = $Periods
          and [Key Account Managers] <> 'legacy k' and [Key Account Managers] is not null
          and [Business Partners] is not null
    sm: null
    expected_ms:
      - mapping_sql_template: |
          select
              [Versions],
              [Periods],
              [Key Account Managers],
              [Business Partners],
              [Measures Key Account Manager to Business Partner],
              [Value]
          from [PlanData].[dbo].[IFL_FACT_RESTATED_KAM_TO_BP]
          where [Periods] = $Periods
          and [Key Account Managers] <> 'legacy k' and [Key Account Managers] is not null
          and [Business Partners] is not null
        mapping_sql_query: |
          select
              [Versions],
              [Periods],
              [Key Account Managers],
              [Business Partners],
              [Measures Key Account Manager to Business Partner],
              [Value]
          from [PlanData].[dbo].[IFL_FACT_RESTATED_KAM_TO_BP]
          where [Periods] = 202202
          and [Key Account Managers] <> 'legacy k' and [Key Account Managers] is not null
          and [Business Partners] is not null
    expected_sm: null


test_normalize_level_column_success:
  -
    # test edges_df schema generation with all columns provided
    input_df:
      {
        "Level1": [ All Regions, null, null, null, null, null, null, null, All Regions, null, null, null ],
        "Level2": [ null, EMEA, null, null, AMER, null, null, null, null, EU, null, null ],
        "Level3": [ null, null, Hungary, Germany, null, USA, null, null, null, null, Hungary, Germany ],
        "Level4": [ null, null, null, null, null, null, California, New York, null, null, null, null ],
        "ElementType": ["C", "C", "N", "N", "C", "C", "N", "N", "C", "C", "N", "N"],
        "Weight": [ 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0 ],
        "Hierarchy": [ "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Alt", "Alt", "Alt", "Alt" ]
      }
    edges_df_expected:
      {
        "Parent": [
          null, "All Regions", "EMEA", "EMEA", "All Regions", "AMER", "USA", "USA",
          null, "All Regions", "EU", "EU"
        ],
        "Child": [
          "All Regions", "EMEA", "Hungary", "Germany", "AMER", "USA", "California", "New York",
          "All Regions", "EU", "Hungary", "Germany"
        ],
        "ElementType":
          ["C", "C", "N", "N", "C", "C", "N", "N",
           "C", "C", "N", "N"],
        "Weight": [
          1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
          1.0, 1.0, 1.0, 1.0
        ],
        "Hierarchy": [
          "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default",
          "Alt", "Alt", "Alt", "Alt"
        ]
      }
  -
    # test weight column addition
    input_df:
      {
        "Level1": [ All Regions, null, null, null, null, null, null, null, All Regions, null, null, null ],
        "Level2": [ null, EMEA, null, null, AMER, null, null, null, null, EU, null, null ],
        "Level3": [ null, null, Hungary, Germany, null, USA, null, null, null, null, Hungary, Germany ],
        "Level4": [ null, null, null, null, null, null, California, New York, null, null, null, null ],
        "ElementType": [ "C", "C", "N", "N", "C", "C", "N", "N", "C", "C", "N", "N" ],
        "Hierarchy": [ "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Alt", "Alt", "Alt", "Alt" ]
      }
    edges_df_expected:
      {
        "Parent": [
          null, "All Regions", "EMEA", "EMEA", "All Regions", "AMER", "USA", "USA",
          null, "All Regions", "EU", "EU"
        ],
        "Child": [
          "All Regions", "EMEA", "Hungary", "Germany", "AMER", "USA", "California", "New York",
          "All Regions", "EU", "Hungary", "Germany"
        ],
        "ElementType":
          [ "C", "C", "N", "N", "C", "C", "N", "N",
            "C", "C", "N", "N" ],
        "Weight": [
          1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
          1.0, 1.0, 1.0, 1.0
        ],
        "Hierarchy": [
          "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default",
          "Alt", "Alt", "Alt", "Alt"
        ]
      }
  -
    # test hierarchy column addition
    input_df:
      {
        "Level1": [ All Regions, null, null, null, null, null, null, null],
        "Level2": [ null, EMEA, null, null, AMER, null, null, null],
        "Level3": [ null, null, Hungary, Germany, null, USA, null, null],
        "Level4": [ null, null, null, null, null, null, California, New York],
        "ElementType": [ "C", "C", "N", "N", "C", "C", "N", "N"],
        "Weight": [ 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
      }
    edges_df_expected:
      {
        "Parent": [
          null, "All Regions", "EMEA", "EMEA", "All Regions", "AMER", "USA", "USA"
        ],
        "Child": [
          "All Regions", "EMEA", "Hungary", "Germany", "AMER", "USA", "California", "New York"
        ],
        "ElementType": [
          "C", "C", "N", "N", "C", "C", "N", "N"
        ],
        "Weight": [
          1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0
        ],
        "Hierarchy": [
          "Country", "Country", "Country", "Country", "Country", "Country", "Country", "Country"
        ]
      }
  -
    # test for missing value defaulting
    input_df:
      {
        "Level1": [ All Regions, null, null, null, null, null, null, null, All Regions, null, null, null ],
        "Level2": [ null, EMEA, null, null, AMER, null, null, null, null, EU, null, null ],
        "Level3": [ null, null, Hungary, Germany, null, USA, null, null, null, null, Hungary, Germany ],
        "Level4": [ null, null, null, null, null, null, California, New York, null, null, null, null ],
        "ElementType": [ "C", "C", "N", "N", "C", "C", "N", "N", "C", "C", "N", "N" ],
        "Weight": [ 1.0, null, 1.0, 1.0, 1.0, 1.0, 1.0, null, 1.0, 1.0, 1.0, 1.0 ],
        "Hierarchy": [ "Country", "Country", "Country", "Country", null, null, "Country", "Country", "Alt", "Alt", "Alt", "Alt" ]
      }
    edges_df_expected:
      {
        "Parent": [
          null, "All Regions", "EMEA", "EMEA", "All Regions", "AMER", "USA", "USA",
          null, "All Regions", "EU", "EU"
        ],
        "Child": [
          "All Regions", "EMEA", "Hungary", "Germany", "AMER", "USA", "California", "New York",
          "All Regions", "EU", "Hungary", "Germany"
        ],
        "ElementType":
          [ "C", "C", "N", "N", "C", "C", "N", "N",
            "C", "C", "N", "N" ],
        "Weight": [
          1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
          1.0, 1.0, 1.0, 1.0
        ],
        "Hierarchy": [
          "Country", "Country", "Country", "Country", "Country", "Country", "Country", "Country",
          "Alt", "Alt", "Alt", "Alt"
        ]
      }
  -
    # test for number to float conversion
    input_df:
      {
        "Level1": [ All Regions, null, null, null, null, null, null, null, All Regions, null, null, null ],
        "Level2": [ null, EMEA, null, null, AMER, null, null, null, null, EU, null, null ],
        "Level3": [ null, null, Hungary, Germany, null, USA, null, null, null, null, Hungary, Germany ],
        "Level4": [ null, null, null, null, null, null, California, New York, null, null, null, null ],
        "ElementType": [ "C", "C", "N", "N", "C", "C", "N", "N", "C", "C", "N", "N" ],
        "Weight": [ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ],
        "Hierarchy": [ "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Alt", "Alt", "Alt", "Alt" ]
      }
    edges_df_expected:
      {
        "Parent": [
          null, "All Regions", "EMEA", "EMEA", "All Regions", "AMER", "USA", "USA",
          null, "All Regions", "EU", "EU"
        ],
        "Child": [
          "All Regions", "EMEA", "Hungary", "Germany", "AMER", "USA", "California", "New York",
          "All Regions", "EU", "Hungary", "Germany"
        ],
        "ElementType":
          [ "C", "C", "N", "N", "C", "C", "N", "N",
            "C", "C", "N", "N" ],
        "Weight": [
          1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
          1.0, 1.0, 1.0, 1.0
        ],
        "Hierarchy": [
          "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default",
          "Alt", "Alt", "Alt", "Alt"
        ]
      }

test_normalize_level_column_fail:
  -
    input_df:
      {
        "Level1": [ All Regions, null, null, null, null, null, null, null, All Regions, null, null, null ],
        "Level2": [ null, EMEA, null, null, AMER, null, null, null, null, EU, null, null ],
        "Level3": [ null, null, null, Germany, null, USA, null, null, null, null, Hungary, Germany ],
        "Level4": [ null, null, null, null, null, null, California, New York, null, null, null, null ],
        "ElementType": ["C", "C", "N", "N", "C", "C", "N", "N", "C", "C", "N", "N"],
        "Weight": [ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ],
        "Hierarchy": [ "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Alt", "Alt", "Alt", "Alt" ]
      }
    expected_exception: "LevelColumnInvalidRowError"
    expected_errormessage: "Empty row, no element found"
  -
    input_df:
      {
        "Level1": [ All Regions, null, null, null, null, null, null, null, All Regions, null, null, null ],
        "Level2": [ null, EMEA, null, null, AMER, null, null, null, null, EU, null, null ],
        "Level3": [ null, null, Hungary, Germany, null, USA, null, null, null, null, Hungary, Germany ],
        "Level4": [ null, null, Budapest, null, null, null, California, New York, null, null, null, null ],
        "ElementType": [ "C", "C", "N", "N", "C", "C", "N", "N", "C", "C", "N", "N" ],
        "Weight": [ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ],
        "Hierarchy": [ "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Alt", "Alt", "Alt", "Alt" ]
      }
    expected_exception: "LevelColumnInvalidRowError"
    expected_errormessage: "Multiple elements found"
  -
    input_df:
      {
        "Level1": [ All Regions, null, null, null, null, null, null, null, All Regions, null, null, null ],
        "Level2": [ null, EMEA, null, null, AMER, null, null, null, null, EU, null, null ],
        "Level3": [ null, null, null, Germany, null, USA, null, null, null, null, Hungary, Germany ],
        "Level4": [ null, null, Hungary, null, null, null, California, New York, null, null, null, null ],
        "ElementType": [ "C", "C", "N", "N", "C", "C", "N", "N", "C", "C", "N", "N" ],
        "Weight": [ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ],
        "Hierarchy": [ "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Alt", "Alt", "Alt", "Alt" ]
      }
    expected_exception: "LevelColumnInvalidRowError"
    expected_errormessage: "Missing parent of child element"

test_normalize_parent_child_success:
  -
    input_df:
      {
        "Parent": [
          null, "All Regions", "EMEA", "EMEA", "All Regions", "AMER", "USA", "USA"
        ],
        "Child": [
          "All Regions", "EMEA", "Hungary", "Germany", "AMER", "USA", "California", "New York"
        ],
        "ElementType": [
          "C", "C", "N", "N", "C", "C", "N", "N"
        ],
        "Hierarchy": [
          "Country", "Country", "Country", "Country", "Country", "Country", "Country", "Country"
        ]
      }
    edges_df_expected:
      {
        "Parent": [
          null, "All Regions", "EMEA", "EMEA", "All Regions", "AMER", "USA", "USA"
        ],
        "Child": [
          "All Regions", "EMEA", "Hungary", "Germany", "AMER", "USA", "California", "New York"
        ],
        "ElementType": [
          "C", "C", "N", "N", "C", "C", "N", "N"
        ],
        "Weight": [
          1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0
        ],
        "Hierarchy": [
          "Country", "Country", "Country", "Country", "Country", "Country", "Country", "Country"
        ]
      }
  -
    input_df:
      {
        "Parent": [
          null, "All Regions", "EMEA", "EMEA", "All Regions", "AMER", "USA", "USA"
        ],
        "Child": [
          "All Regions", "EMEA", "Hungary", "Germany", "AMER", "USA", "California", "New York"
        ],
        "ElementType": [
          "C", "C", "N", "N", "C", "C", "N", "N"
        ],
        "Weight": [
          1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0
        ]
      }
    edges_df_expected:
      {
        "Parent": [
          null, "All Regions", "EMEA", "EMEA", "All Regions", "AMER", "USA", "USA"
        ],
        "Child": [
          "All Regions", "EMEA", "Hungary", "Germany", "AMER", "USA", "California", "New York"
        ],
        "ElementType": [
          "C", "C", "N", "N", "C", "C", "N", "N"
        ],
        "Weight": [
          1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0
        ],
        "Hierarchy": [
          "Country", "Country", "Country", "Country", "Country", "Country", "Country", "Country"
        ]
      }
  -
    input_df:
      {
        "Parent": [
          null, "All Regions", "EMEA", "EMEA", "All Regions", "AMER", "USA", "USA"
        ],
        "Child": [
          "All Regions", "EMEA", "Hungary", "Germany", "AMER", "USA", "California", "New York"
        ],
        "ElementType": [
          "C", "C", "N", "N", "C", "C", "N", "N"
        ],
        "Weight": [
          1.0, 1.0, null, 1.0, null, 1.0, 1.0, 1.0
        ],
        "Hierarchy": [
          "Country", "Country", null, "Country", null, "Country", "Country", "Country"
        ]
      }
    edges_df_expected:
      {
        "Parent": [
          null, "All Regions", "EMEA", "EMEA", "All Regions", "AMER", "USA", "USA"
        ],
        "Child": [
          "All Regions", "EMEA", "Hungary", "Germany", "AMER", "USA", "California", "New York"
        ],
        "ElementType": [
          "C", "C", "N", "N", "C", "C", "N", "N"
        ],
        "Weight": [
          1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0
        ],
        "Hierarchy": [
          "Country", "Country", "Country", "Country", "Country", "Country", "Country", "Country"
        ]
      }


test_normalize_all_column_names:
  - # Scenario 1: Normalize all possible columns
    input_df: {
      "d_name": ["Product", "Product"],
      "h_name": ["Standard", "Standard"],
      "p_name": ["Total", "Total"],
      "c_name": ["Bike", "Car"],
      "t_name": ["Consol", "Consol"],
      "w_name": [1, 1]
    }
    dim_column: "d_name"
    hier_column: "h_name"
    parent_column: "p_name"
    child_column: "c_name"
    element_column: null
    type_column: "t_name"
    weight_column: "w_name"
    expected_df: {
      "Dimension": ["Product", "Product"],
      "Hierarchy": ["Standard", "Standard"],
      "Parent": ["Total", "Total"],
      "Child": ["Bike", "Car"],
      "ElementType": ["Consol", "Consol"],
      "Weight": [1, 1]
    }

  - # Scenario 2: Using element_column instead of child_column
    input_df: {
      "el": ["Item1", "Item2"],
      "other": [1, 2]
    }
    dim_column: null
    hier_column: null
    parent_column: null
    child_column: null
    element_column: "el"
    type_column: null
    weight_column: null
    expected_df: {
      "Child": ["Item1", "Item2"],
      "other": [1, 2]
    }

  - # Scenario 3: Column name provided but does not exist in DF (should do nothing)
    input_df: {
      "RealColumn": [10, 20]
    }
    dim_column: "FakeColumn"
    hier_column: null
    parent_column: null
    child_column: null
    element_column: null
    type_column: null
    weight_column: null
    expected_df: {
      "RealColumn": [10, 20]
    }

test_assign_missing_edge_columns:
  - # Scenario 1: All edge columns missing (Hierarchy defaults to Dimension)
    input_df: {
      "Parent": ["Total", "Total"],
      "Child": ["A", "B"]
    }
    dimension_name: "Product"
    hierarchy_name: null
    expected_df: {
      "Parent": ["Total", "Total"],
      "Child": ["A", "B"],
      "Weight": [1.0, 1.0],
      "Dimension": ["Product", "Product"],
      "Hierarchy": ["Product", "Product"]
    }

  - # Scenario 2: All edge columns missing (Hierarchy provided)
    input_df: {
      "Parent": ["Region"],
      "Child": ["North"]
    }
    dimension_name: "Geography"
    hierarchy_name: "SalesTerritory"
    expected_df: {
      "Parent": ["Region"],
      "Child": ["North"],
      "Weight": [1.0],
      "Dimension": ["Geography"],
      "Hierarchy": ["SalesTerritory"]
    }

  - # Scenario 3: Some columns already exist (Should NOT be overwritten)
    input_df: {
      "Parent": ["All"],
      "Child": ["Item"],
      "Weight": [2.5],
      "Dimension": ["ExistingDim"]
    }
    dimension_name: "NewDim"
    hierarchy_name: "NewHier"
    expected_df: {
      "Parent": ["All"],
      "Child": ["Item"],
      "Weight": [2.5],
      "Dimension": ["ExistingDim"],
      "Hierarchy": ["NewHier"]
    }

  - # Scenario 4: All columns exist (Function should return DF unchanged)
    input_df: {
      "Dimension": ["D1"],
      "Hierarchy": ["H1"],
      "Weight": [5.0]
    }
    dimension_name: "IgnoreMe"
    hierarchy_name: "IgnoreMe"
    expected_df: {
      "Dimension": ["D1"],
      "Hierarchy": ["H1"],
      "Weight": [5.0]
    }

test_assign_parent_child_to_level_columns:
  - # Scenario 1: Both Parent and Child are missing
    input_df: {
      "Dimension": ["Product", "Product"],
      "Weight": [1.0, 1.0]
    }
    expected_df: {
      "Dimension": ["Product", "Product"],
      "Weight": [1.0, 1.0],
      "Parent": ["", ""],
      "Child": ["", ""]
    }

  - # Scenario 2: Parent exists, but Child is missing
    input_df: {
      "Parent": ["Total", "Total"],
      "Other": [1, 2]
    }
    expected_df: {
      "Parent": ["Total", "Total"],
      "Other": [1, 2],
      "Child": ["", ""]
    }

  - # Scenario 3: Child exists, but Parent is missing
    input_df: {
      "Child": ["ElementA"],
      "Value": [100]
    }
    expected_df: {
      "Child": ["ElementA"],
      "Value": [100],
      "Parent": [""]
    }

  - # Scenario 4: Both already exist (Should remain unchanged)
    input_df: {
      "Parent": ["P1"],
      "Child": ["C1"]
    }
    expected_df: {
      "Parent": ["P1"],
      "Child": ["C1"]
    }

test_fill_column_empty_values_with_defaults:
  - # Scenario 1: Fill empty and whitespace strings with a string default
    input_df: {
      "Hierarchy": ["Actuals", "", "  ", "Budget"]
    }
    column_name: "Hierarchy"
    default_value: "General"
    expected_df: {
      "Hierarchy": ["Actuals", "General", "General", "Budget"]
    }

  - # Scenario 2: Fill actual NaN/None values with a numeric default
    input_df: {
      "Weight": [1.0, null, 2.5]
    }
    column_name: "Weight"
    default_value: 0.0
    expected_df: {
      "Weight": [1.0, 0.0, 2.5]
    }

  - # Scenario 3: Mixed content in other columns (should remain untouched)
    input_df: {
      "ID": [1, 2],
      "Comment": ["", "Valid"]
    }
    column_name: "Comment"
    default_value: "N/A"
    expected_df: {
      "ID": [1, 2],
      "Comment": ["N/A", "Valid"]
    }

  - # Scenario 4: Column with no empty values (should remain unchanged)
    input_df: {
      "Status": ["Active", "Inactive"]
    }
    column_name: "Status"
    default_value: "Unknown"
    expected_df: {
      "Status": ["Active", "Inactive"]
    }

test_assign_missing_edge_values:
  - # Scenario 1: Fill empty strings and nulls (Hierarchy provided)
    input_df: {
      "Dimension": ["", null],
      "Hierarchy": ["", " "],
      "Weight": [null, ""]
    }
    dimension_name: "Product"
    hierarchy_name: "Standard"
    expected_df: {
      "Dimension": ["Product", "Product"],
      "Hierarchy": ["Standard", "Standard"],
      "Weight": [1.0, 1.0]
    }

  - # Scenario 2: Hierarchy name is None (should default to Dimension name)
    input_df: {
      "Dimension": [null],
      "Hierarchy": [""],
      "Weight": [null]
    }
    dimension_name: "Geography"
    hierarchy_name: null
    expected_df: {
      "Dimension": ["Geography"],
      "Hierarchy": ["Geography"],
      "Weight": [1.0]
    }

  - # Scenario 3: Mixed values (Partial fill)
    input_df: {
      "Dimension": ["ExistingDim", ""],
      "Hierarchy": ["ExistingHier", null],
      "Weight": [5.0, null],
      "Other": ["Keep", "Me"]
    }
    dimension_name: "NewDim"
    hierarchy_name: "NewHier"
    expected_df: {
      "Dimension": ["ExistingDim", "NewDim"],
      "Hierarchy": ["ExistingHier", "NewHier"],
      "Weight": [5.0, 1.0],
      "Other": ["Keep", "Me"]
    }

  - # Scenario 4: All values already valid (No changes)
    input_df: {
      "Dimension": ["A"],
      "Hierarchy": ["B"],
      "Weight": [2.0]
    }
    dimension_name: "X"
    hierarchy_name: "Y"
    expected_df: {
      "Dimension": ["A"],
      "Hierarchy": ["B"],
      "Weight": [2.0]
    }

test_assign_missing_type_column:
  - # Scenario 1: Column is missing
    input_df: {
      "Child": ["Item1", "Item2"],
      "Weight": [1.0, 1.0]
    }
    expected_df: {
      "Child": ["Item1", "Item2"],
      "Weight": [1.0, 1.0],
      "ElementType": ["", ""]
    }

  - # Scenario 2: Column already exists (Should NOT be overwritten)
    input_df: {
      "Child": ["Item1"],
      "ElementType": ["Numeric"]
    }
    expected_df: {
      "Child": ["Item1"],
      "ElementType": ["Numeric"]
    }

  - # Scenario 3: Column exists but is empty/null (Should NOT be overwritten)
    # The function only checks if the column name exists, not its content.
    input_df: {
      "Child": ["Item1", "Item2"],
      "ElementType": [null, "Consolidated"]
    }
    expected_df: {
      "Child": ["Item1", "Item2"],
      "ElementType": [null, "Consolidated"]
    }

test_assign_missing_type_values:
  - # Scenario 1: Fill empty values based on parent/child relationships
    # "SubTotal" is a Child but also appears in the Parent list -> gets 'N'
    # "LeafA" is a Child but never appears in the Parent list -> gets 'C'
    input_df: {
      "Parent": ["Total", "SubTotal"],
      "Child": ["SubTotal", "LeafA"],
      "ElementType": ["", null]
    }
    expected_df: {
      "Parent": ["Total", "SubTotal"],
      "Child": ["SubTotal", "LeafA"],
      "ElementType": ["N", "C"]
    }

  - # Scenario 2: Preservation of existing values
    # Existing values like 'Existing' should not be overwritten even if empty-check would trigger
    input_df: {
      "Parent": ["Total"],
      "Child": ["SubTotal"],
      "ElementType": ["Existing"]
    }
    expected_df: {
      "Parent": ["Total"],
      "Child": ["SubTotal"],
      "ElementType": ["Existing"]
    }

  - # Scenario 3: Mixed empty types (None, NaN, "")
    input_df: {
      "Parent": ["A", "A", "B"],
      "Child": ["B", "C", "D"],
      "ElementType": [null, "", null]
    }
    expected_df: {
      "Parent": ["A", "A", "B"],
      "Child": ["B", "C", "D"],
      "ElementType": ["N", "C", "C"]
    }

  - # Scenario 4: Whitespace check
    # Note: The code .isin([np.nan, None, ""]) does NOT catch " " (spaces).
    # This test confirms that behavior.
    input_df: {
      "Parent": ["Total"],
      "Child": ["Leaf"],
      "ElementType": [" "]
    }
    expected_df: {
      "Parent": ["Total"],
      "Child": ["Leaf"],
      "ElementType": [" "]
    }

test_separate_edge_df_columns:
  - # Scenario 1: Filter out extra columns
    input_df: {
      "Parent": ["Total", "Total"],
      "Child": ["A", "B"],
      "Weight": [1.0, 1.0],
      "Dimension": ["Product", "Product"],
      "Hierarchy": ["Standard", "Standard"],
      "Extra_Data": [100, 200],
      "Calculation_Flag": [true, false]
    }
    expected_df: {
      "Parent": ["Total", "Total"],
      "Child": ["A", "B"],
      "Weight": [1.0, 1.0],
      "Dimension": ["Product", "Product"],
      "Hierarchy": ["Standard", "Standard"]
    }

  - # Scenario 2: Reorder columns to the standard sequence
    # The input has all required columns but in a random order
    input_df: {
      "Dimension": ["Geo"],
      "Hierarchy": ["Default"],
      "Parent": ["World"],
      "Weight": [1.0],
      "Child": ["Europe"]
    }
    expected_df: {
      "Parent": ["World"],
      "Child": ["Europe"],
      "Weight": [1.0],
      "Dimension": ["Geo"],
      "Hierarchy": ["Default"]
    }

  - # Scenario 3: Mixed types and data integrity
    input_df: {
      "Parent": ["P1"],
      "Child": ["C1"],
      "Weight": [0.5],
      "Dimension": ["D1"],
      "Hierarchy": ["H1"],
      "Unwanted": [null]
    }
    expected_df: {
      "Parent": ["P1"],
      "Child": ["C1"],
      "Weight": [0.5],
      "Dimension": ["D1"],
      "Hierarchy": ["H1"]
    }

test_separate_attr_df_columns:
  - # Scenario 1: Only base columns (attr_columns is None)
    input_df: {
      "Child": ["Item1"],
      "ElementType": ["N"],
      "Dimension": ["Product"],
      "Hierarchy": ["Standard"],
      "Extra_Col": ["Ignore"]
    }
    attr_columns: null
    expected_df: {
      "ElementName": ["Item1"],
      "ElementType": ["N"],
      "Dimension": ["Product"],
      "Hierarchy": ["Standard"]
    }

  - # Scenario 2: Base columns plus specific attribute columns
    input_df: {
      "Child": ["Item1", "Item2"],
      "ElementType": ["N", "C"],
      "Dimension": ["Product", "Product"],
      "Hierarchy": ["Standard", "Standard"],
      "Color": ["Red", "Blue"],
      "Size": [10, 20],
      "Weight": [1.0, 1.0]
    }
    attr_columns: ["Color", "Size"]
    expected_df: {
      "ElementName": ["Item1", "Item2"],
      "ElementType": ["N", "C"],
      "Dimension": ["Product", "Product"],
      "Hierarchy": ["Standard", "Standard"],
      "Color": ["Red", "Blue"],
      "Size": [10, 20]
    }

  - # Scenario 3: Verify Column Renaming and Subset
    # Input has columns in different order; output should follow base + attr order
    input_df: {
      "Dimension": ["D1"],
      "Hierarchy": ["H1"],
      "Child": ["C1"],
      "ElementType": ["T1"],
      "Price": [100]
    }
    attr_columns: ["Price"]
    expected_df: {
      "ElementName": ["C1"],
      "ElementType": ["T1"],
      "Dimension": ["D1"],
      "Hierarchy": ["H1"],
      "Price": [100]
    }

test_create_stack:
  - # Scenario 1: Multiple unique hierarchies
    input_df: {
      "Hierarchy": ["Standard", "Management", "Standard"],
      "Child": ["A", "B", "C"]
    }
    expected_stack: {
      "Standard": {},
      "Management": {}
    }

  - # Scenario 2: Single hierarchy with multiple rows
    input_df: {
      "Hierarchy": ["Actuals", "Actuals", "Actuals"],
      "Value": [10, 20, 30]
    }
    expected_stack: {
      "Actuals": {}
    }

  - # Scenario 3: Hierarchies with special characters or spaces
    input_df: {
      "Hierarchy": ["Sales 2024", "Profit & Loss"]
    }
    expected_stack: {
      "Sales 2024": {},
      "Profit & Loss": {}
    }

test_update_stack:
  - # Scenario 1: Add a new level to an empty hierarchy
    stack: {
      "Standard": {}
    }
    hierarchy: "Standard"
    element_level: 0
    element_name: "Total Product"
    expected_stack: {
      "Standard": {0: "Total Product"}
    }

  - # Scenario 2: Add a deeper level (no deletion required)
    stack: {
      "Standard": {0: "Total Product"}
    }
    hierarchy: "Standard"
    element_level: 1
    element_name: "Electronics"
    expected_stack: {
      "Standard": {0: "Total Product", 1: "Electronics"}
    }

  - # Scenario 3: Update an existing level and prune higher levels
    # Since element_level is 1, level 2 should be deleted
    stack: {
      "Standard": {0: "Total Product", 1: "Electronics", 2: "Phones"}
    }
    hierarchy: "Standard"
    element_level: 1
    element_name: "Furniture"
    expected_stack: {
      "Standard": {0: "Total Product", 1: "Furniture"}
    }

  - # Scenario 4: Update level 0 (prune all subsequent levels)
    stack: {
      "Standard": {0: "Total Product", 1: "Electronics", 2: "Phones"}
    }
    hierarchy: "Standard"
    element_level: 0
    element_name: "New Root"
    expected_stack: {
      "Standard": {0: "New Root"}
    }

  - # Scenario 5: Multiple hierarchies (ensure only the target is modified)
    stack: {
      "H1": {0: "Root1", 1: "Child1"},
      "H2": {0: "Root2", 1: "Child2"}
    }
    hierarchy: "H1"
    element_level: 0
    element_name: "Changed"
    expected_stack: {
      "H1": { 0: "Changed" },
      "H2": { 0: "Root2", 1: "Child2" }
    }

test_parse_indented_level_columns:
  - # Scenario 1: Element at the first level (level 0)
    input_row: {
      "Level0": "Total Company",
      "Level1": "",
      "Level2": ""
    }
    row_index: 0
    level_columns: ["Level0", "Level1", "Level2"]
    expected_name: "Total Company"
    expected_level: 0

  - # Scenario 2: Element at a deeper level (level 2)
    input_row: {
      "Level0": "",
      "Level1": null,
      "Level2": "Marketing Department"
    }
    row_index: 10
    level_columns: ["Level0", "Level1", "Level2"]
    expected_name: "Marketing Department"
    expected_level: 2

  - # Scenario 3: Mixed empty strings and None values
    input_row: {
      "L1": null,
      "L2": "Product A",
      "L3": ""
    }
    row_index: "row_5"
    level_columns: ["L1", "L2", "L3"]
    expected_name: "Product A"
    expected_level: 1

  - # Scenario 4: Row with values in columns not specified in level_columns
    input_row: {
      "Level0": "",
      "Level1": "Sales",
      "OtherColumn": "IgnoreMe"
    }
    row_index: 1
    level_columns: ["Level0", "Level1"]
    expected_name: "Sales"
    expected_level: 1

test_parse_indented_level_columns_failure:
  - # Scenario: Multiple elements in a single row
    input_row: {
      "Level0": "Total Company",
      "Level1": "Marketing",
      "Level2": ""
    }
    row_index: 5
    level_columns: ["Level0", "Level1", "Level2"]
    expected_exception: "LevelColumnInvalidRowError"
    expected_message: "Multiple elements found. Exactly one is expected."

  - # Scenario: No elements in the row
    input_row: {
      "Level0": "",
      "Level1": null,
      "Level2": ""
    }
    row_index: 10
    level_columns: ["Level0", "Level1", "Level2"]
    expected_exception: "LevelColumnInvalidRowError"
    expected_message: "Empty row, no element found. Exactly one is expected."

test_parse_filled_level_columns:
  - # Scenario 1: Fully filled row
    input_row: { "L0": "Total", "L1": "Region", "L2": "Store" }
    row_index: 0
    level_columns: ["L0", "L1", "L2"]
    expected_name: "Store"
    expected_level: 2

  - # Scenario 2: Partially filled (continuous from start)
    input_row: { "L0": "Total", "L1": "Electronics", "L2": "" }
    row_index: 1
    level_columns: ["L0", "L1", "L2"]
    expected_name: "Electronics"
    expected_level: 1

  - # Scenario 3: Only first level filled
    input_row: { "L0": "Company", "L1": null }
    row_index: 2
    level_columns: ["L0", "L1"]
    expected_name: "Company"
    expected_level: 0

test_parse_filled_level_columns_failure:
  - # Scenario 1: Completely empty row (triggers count validation)
    input_row: {
      "L0": "",
      "L1": null,
      "L2": ""
    }
    row_index: 101
    level_columns: ["L0", "L1", "L2"]
    expected_exception: "LevelColumnInvalidRowError"
    expected_message: "Empty row, no element found. Exactly one is expected."

  - # Scenario 2: Gap at the start (triggers fill validation)
    input_row: {
      "L0": "",
      "L1": "Product A",
      "L2": ""
    }
    row_index: 102
    level_columns: ["L0", "L1", "L2"]
    expected_exception: "LevelColumnInvalidRowError"
    expected_message: "Row has a gap: level is filled but a previous level was empty."

  - # Scenario 3: Gap in the middle (triggers fill validation)
    input_row: {
      "L0": "Total Company",
      "L1": "",
      "L2": "Finance Department"
    }
    row_index: 111
    level_columns: ["L0", "L1", "L2"]
    expected_exception: "LevelColumnInvalidRowError"
    expected_message: "Row has a gap: level is filled but a previous level was empty."

test_parse_indented_levels_into_parent_child:
  - # Scenario: Standard indented levels with multiple hierarchies
    input_df: {
      "Hierarchy": ["Standard", "Standard", "Standard", "Regional", "Regional"],
      "L0": ["Total", "", "", "World", ""],
      "L1": ["", "Hardware", "Software", "", "Europe"],
      "Child": [null, null, null, null, null],
      "Parent": [null, null, null, null, null]
    }
    level_columns: ["L0", "L1"]
    expected_df: {
      "Hierarchy": ["Standard", "Standard", "Standard", "Regional", "Regional"],
      "L0": ["Total", "", "", "World", ""],
      "L1": ["", "Hardware", "Software", "", "Europe"],
      "Child": ["Total", "Hardware", "Software", "World", "Europe"],
      "Parent": [null, "Total", "Total", null, "World"]
    }

  - # Scenario: Deeply nested hierarchy
    input_df: {
      "Hierarchy": ["H1", "H1", "H1"],
      "Level 0": ["Grandparent", "", ""],
      "Level 1": ["", "Parent", ""],
      "Level 2": ["", "", "Child"],
      "Child": ["", "", ""],
      "Parent": ["", "", ""]
    }
    level_columns: ["Level 0", "Level 1", "Level 2"]
    expected_df: {
      "Hierarchy": ["H1", "H1", "H1"],
      "Level 0": ["Grandparent", "", ""],
      "Level 1": ["", "Parent", ""],
      "Level 2": ["", "", "Child"],
      "Child": ["Grandparent", "Parent", "Child"],
      "Parent": [null, "Grandparent", "Parent"]
    }

test_parse_indented_levels_into_parent_child_failure:
  - # Scenario 1: First row starts at Level 1 (Missing Level 0 parent)
    input_df: {
      "Hierarchy": ["Standard"],
      "L0": [""],
      "L1": ["Child Without Parent"],
      "Child": [null],
      "Parent": [null]
    }
    level_columns: ["L0", "L1"]
    expected_exception: "LevelColumnInvalidRowError"
    expected_message: "Missing parent of child element"

  - # Scenario 2: Jumping levels (Level 0 exists, but row 2 jumps to Level 2)
    input_df: {
      "Hierarchy": ["Standard", "Standard"],
      "L0": ["Root", ""],
      "L1": ["", ""],
      "L2": ["", "Jumping Level"],
      "Child": [null, null],
      "Parent": [null, null]
    }
    level_columns: ["L0", "L1", "L2"]
    expected_exception: "LevelColumnInvalidRowError"
    expected_message: "Missing parent of child element"

  - # Scenario 3: Missing parent in a second hierarchy
    input_df: {
      "Hierarchy": ["H1", "H1", "H2"],
      "L0": ["Root1", "", ""],
      "L1": ["", "Child1", "Child2"],
      "Child": [null, null, null],
      "Parent": [null, null, null]
    }
    level_columns: ["L0", "L1"]
    expected_exception: "LevelColumnInvalidRowError"
    expected_message: "Missing parent of child element"
  - # Scenario 4: Missing level columns
    input_df: {
      "Hierarchy": [ "H1", "H1", "H2" ],
      "L0": [ "Root1", "", "" ],
      "L1": [ "", "Child1", "Child2" ],
      "Child": [ null, null, null ],
      "Parent": [ null, null, null ]
    }
    level_columns: [ "Level0", "L1" ]
    expected_exception: "SchemaValidationError"
    expected_message: "Level column Level0 is missing."

test_parse_filled_levels_into_parent_child:
  - # Scenario 1: Standard filled hierarchy with varying depths
    input_df: {
      "L0": ["Total", "Total", "Total"],
      "L1": ["", "Hardware", "Hardware"],
      "L2": ["", "", "CPU"],
      "Child": [null, null, null],
      "Parent": [null, null, null]
    }
    level_columns: ["L0", "L1", "L2"]
    expected_df: {
      "L0": ["Total", "Total", "Total"],
      "L1": ["", "Hardware", "Hardware"],
      "L2": ["", "", "CPU"],
      "Child": ["Total", "Hardware", "CPU"],
      "Parent": ["", "Total", "Hardware"]
    }

  - # Scenario 2: Multiple branches in the same table
    input_df: {
      "L0": ["Company", "Company", "Company", "Company"],
      "L1": ["", "Sales", "Sales", "HR"],
      "L2": ["", "", "West", ""],
      "Child": ["", "", "", ""],
      "Parent": ["", "", "", ""]
    }
    level_columns: ["L0", "L1", "L2"]
    expected_df: {
      "L0": ["Company", "Company", "Company", "Company"],
      "L1": ["", "Sales", "Sales", "HR"],
      "L2": ["", "", "West", ""],
      "Child": ["Company", "Sales", "West", "HR"],
      "Parent": ["", "Company", "Sales", "Company"]
    }

  - # Scenario 3: Single level only
    input_df: {
      "Level_0": ["Root1", "Root2"],
      "Child": [null, null],
      "Parent": [null, null]
    }
    level_columns: ["Level_0"]
    expected_df: {
      "Level_0": ["Root1", "Root2"],
      "Child": ["Root1", "Root2"],
      "Parent": ["", ""]
    }

test_parse_filled_levels_into_parent_child_failure:
  - # missing level columns
    input_df: {
      "Hierarchy": [ "H1", "H1", "H2" ],
      "L0": [ "Root1", "Root1", "Root2" ],
      "L1": [ "", "Child1", "Child2" ],
      "Child": [ null, null, null ],
      "Parent": [ null, null, null ]
    }
    level_columns: [ "Level0", "L1" ]
    expected_exception: "SchemaValidationError"
    expected_message: "Level column Level0 is missing."

test_drop_invalid_edges_df_rows:
  - # Scenario 1: Mixed valid and invalid parents
    # Row 0: Valid -> Kept
    # Row 1: Empty string -> Dropped
    # Row 2: null (NaN) -> Dropped
    input_df: {
      "Parent": ["Total", "", null],
      "Child": ["Hardware", "Software", "Services"],
      "Weight": [1.0, 1.0, 1.0]
    }
    expected_df: {
      "Parent": ["Total"],
      "Child": ["Hardware"],
      "Weight": [1.0]
    }

  - # Scenario 2: Multiple valid rows
    input_df: {
      "Parent": ["P1", "P1", ""],
      "Child": ["C1", "C2", "C3"]
    }
    expected_df: {
      "Parent": ["P1", "P1"],
      "Child": ["C1", "C2"]
    }

  - # Scenario 3: All rows are invalid (Result should be empty)
    input_df: {
      "Parent": ["", ""],
      "Child": ["A", "B"]
    }
    expected_df: {
      "Parent": [],
      "Child": []
    }

test_drop_invalid_attr_df_rows:
  - # Scenario 1: Exact duplicates across all columns
    input_df: {
      "ElementName": ["Item1", "Item1", "Item2"],
      "Dimension": ["Product", "Product", "Product"],
      "Hierarchy": ["Standard", "Standard", "Standard"],
      "Attribute_Val": ["Red", "Red", "Blue"]
    }
    expected_df: {
      "ElementName": ["Item1", "Item2"],
      "Dimension": ["Product", "Product"],
      "Hierarchy": ["Standard", "Standard"],
      "Attribute_Val": ["Red", "Blue"]
    }

  - # Scenario 2: Subset duplicate (Keep first)
    # The keys (ElementName, Dimension, Hierarchy) are identical,
    # but Attribute_Val is different. The function should keep the first row.
    input_df: {
      "ElementName": ["Item1", "Item1"],
      "Dimension": ["Dim1", "Dim1"],
      "Hierarchy": ["Hier1", "Hier1"],
      "Attribute_Val": ["First", "Second"]
    }
    expected_df: {
      "ElementName": ["Item1"],
      "Dimension": ["Dim1"],
      "Hierarchy": ["Hier1"],
      "Attribute_Val": ["First"]
    }

  - # Scenario 3: Differentiating by Dimension or Hierarchy
    # Even if ElementName is the same, different Dimension/Hierarchy means they stay.
    input_df: {
      "ElementName": ["Item1", "Item1", "Item1"],
      "Dimension": ["Dim1", "Dim2", "Dim1"],
      "Hierarchy": ["H1", "H1", "H2"]
    }
    expected_df: {
      "ElementName": ["Item1", "Item1", "Item1"],
      "Dimension": ["Dim1", "Dim2", "Dim1"],
      "Hierarchy": ["H1", "H1", "H2"]
    }

test_validate_attr_df_schema_for_inconsistent_element_type_success:
  - # All elements have exactly one type
    df_data:
      - {ElementName: "Elem1", ElementType: "N"}
      - {ElementName: "Elem2", ElementType: "C"}
      - {ElementName: "Elem1", ElementType: "N"} # Duplicate entry is consistent

test_validate_attr_df_schema_for_inconsistent_element_type_failure:
  - # Case: Single inconsistent element
    df_data:
      - {ElementName: "Elem1", ElementType: "N"}
      - {ElementName: "Elem1", ElementType: "S"}
    expected_exception: "SchemaValidationError"
    expected_message_part: "Inconsistency found! These ElementNames have multiple types: ['Elem1']"

  - # Case: Multiple inconsistent elements (alphabetical order check)
    df_data:
      - {ElementName: "Alpha", ElementType: "N"}
      - {ElementName: "Alpha", ElementType: "S"}
      - {ElementName: "Beta", ElementType: "C"}
      - {ElementName: "Beta", ElementType: "N"}
    expected_exception: "SchemaValidationError"
    expected_message_part: "Inconsistency found! These ElementNames have multiple types: ['Alpha', 'Beta']"

test_validate_attr_df_schema_for_inconsistent_leaf_attributes_success:
  - # Case: N elements differ ONLY in Hierarchy or Dimension (Allowed)
    df_data:
      - {ElementName: "Elem1", ElementType: "N", AttributeX: "Val1", Hierarchy: "H1", Dimension: "D1"}
      - {ElementName: "Elem1", ElementType: "N", AttributeX: "Val1", Hierarchy: "H2", Dimension: "D2"}
  - # Case: Non-N/S elements (like 'C') are ignored even if attributes differ
    df_data:
      - {ElementName: "Consol1", ElementType: "C", AttributeX: "Val1", Hierarchy: "H1", Dimension: "D1"}
      - {ElementName: "Consol1", ElementType: "C", AttributeX: "Val2", Hierarchy: "H1", Dimension: "D1"}

test_validate_attr_df_schema_for_inconsistent_leaf_attributes_failure:
  - # Case: Element 'N' has different AttributeX values
    df_data:
      - {ElementName: "LeafA", ElementType: "N", AttributeX: "Red", Hierarchy: "H1"}
      - {ElementName: "LeafA", ElementType: "N", AttributeX: "Blue", Hierarchy: "H2"}
    expected_exception: "SchemaValidationError"
    expected_message_part: "Inconsistency Error: The following ElementNames (type 'N' and 'S') have conflicting values in required columns: ['LeafA']"

  - # Case: Element 'S' has different AttributeX values
    df_data:
      - {ElementName: "StringB", ElementType: "S", AttributeX: "Alpha", Hierarchy: "H1"}
      - {ElementName: "StringB", ElementType: "S", AttributeX: "Beta", Hierarchy: "H1"}
    expected_exception: "SchemaValidationError"
    expected_message_part: "Inconsistency Error: The following ElementNames (type 'N' and 'S') have conflicting values in required columns: ['StringB']"

  - # Case: Verifying that 'Conflicting data:' header is present in the message
    df_data:
      - {ElementName: "Fail", ElementType: "N", AttributeX: "1"}
      - {ElementName: "Fail", ElementType: "N", AttributeX: "2"}
    expected_exception: "SchemaValidationError"
    expected_message_part: "Conflicting data:"

test_validate_graph_for_leaves_as_parents_success:
  - # Case: 'C' (Consolidated) is a parent, 'N' is a child (Valid)
    edges_data:
      - {Parent: "Consol_A", Child: "Leaf_1"}
    attr_data:
      - {ElementName: "Consol_A", ElementType: "C"}
      - {ElementName: "Leaf_1", ElementType: "N"}
  - # Case: N/S elements exist in attributes but are not parents in edges
    edges_data:
      - {Parent: "Total_Sales", Child: "Region_West"}
    attr_data:
      - {ElementName: "Total_Sales", ElementType: "C"}
      - {ElementName: "Unrelated_Leaf", ElementType: "N"}

test_validate_graph_for_leaves_as_parents_failure:
  - # Case: An 'N' type element is used as a Parent
    edges_data:
      - {Parent: "Invalid_Parent", Child: "Some_Child"}
    attr_data:
      - {ElementName: "Invalid_Parent", ElementType: "N"}
      - {ElementName: "Some_Child", ElementType: "N"}
    expected_exception: "GraphValidationError"
    expected_message_part: "The following N/S elements were found as Parents: ['Invalid_Parent']"

  - # Case: An 'S' type element is used as a Parent
    edges_data:
      - {Parent: "String_Node", Child: "Leaf_X"}
    attr_data:
      - {ElementName: "String_Node", ElementType: "S"}
      - {ElementName: "Leaf_X", ElementType: "N"}
    expected_exception: "GraphValidationError"
    expected_message_part: "The following N/S elements were found as Parents: ['String_Node']"

  - # Case: Multiple invalid elements (check list formatting in message)
    edges_data:
      - {Parent: "Bad_N", Child: "Child1"}
      - {Parent: "Bad_S", Child: "Child2"}
    attr_data:
      - {ElementName: "Bad_N", ElementType: "N"}
      - {ElementName: "Bad_S", ElementType: "S"}
    expected_exception: "GraphValidationError"
    expected_message_part: "The following N/S elements were found as Parents: ['Bad_N', 'Bad_S']"

test_validate_graph_for_self_loop_success:
  - # Case: Standard hierarchy
    df_data:
      - {Parent: "A", Child: "B"}
      - {Parent: "B", Child: "C"}

test_validate_graph_for_self_loop_failure:
  - # Case: Single row self-loop
    df_data:
      - {Parent: "Node_A", Child: "Node_A"}
    expected_exception: "GraphValidationError"
    expected_message_part: "A child is the parent of itself, self loop detected."

  - # Case: Self-loop hidden among valid rows
    df_data:
      - {Parent: "Total", Child: "Region"}
      - {Parent: "Region", Child: "Region"} # The loop
      - {Parent: "Region", Child: "Store"}
    expected_exception: "GraphValidationError"
    expected_message_part: "A child is the parent of itself, self loop detected."

  - # Case: Multiple self-loops
    df_data:
      - {Parent: "A", Child: "A"}
      - {Parent: "B", Child: "B"}
    expected_exception: "GraphValidationError"
    expected_message_part: "A child is the parent of itself, self loop detected."

test_validate_graph_for_cycles_with_dfs_success:
  - # Case: Simple chain (A -> B -> C)
    df_data:
      - {Parent: "A", Child: "B"}
      - {Parent: "B", Child: "C"}
  - # Case: Diamond Shape (Valid: A points to B and C, both point to D)
    # This is a key test for DFS to ensure it handles multiple paths correctly.
    df_data:
      - {Parent: "A", Child: "B"}
      - {Parent: "A", Child: "C"}
      - {Parent: "B", Child: "D"}
      - {Parent: "C", Child: "D"}
  - # Case: Multiple separate trees
    df_data:
      - {Parent: "A", Child: "B"}
      - {Parent: "X", Child: "Y"}

test_validate_graph_for_cycles_with_dfs_failure:
  - # Case: Direct cycle (A -> B -> A)
    df_data:
      - {Parent: "A", Child: "B"}
      - {Parent: "B", Child: "A"}
    expected_exception: "GraphValidationError"
    expected_message_part: "Graph contains a cycle starting from node:"

  - # Case: Indirect cycle (A -> B -> C -> A)
    df_data:
      - {Parent: "A", Child: "B"}
      - {Parent: "B", Child: "C"}
      - {Parent: "C", Child: "A"}
    expected_exception: "GraphValidationError"
    expected_message_part: "Graph contains a cycle starting from node:"

  - # Case: Self-loop (A -> A)
    df_data:
      - {Parent: "Self", Child: "Self"}
    expected_exception: "GraphValidationError"
    expected_message_part: "Graph contains a cycle starting from node: Self"

  - # Case: Cycle hidden deep in a hierarchy
    df_data:
      - {Parent: "Root", Child: "Level1"}
      - {Parent: "Level1", Child: "LoopA"}
      - {Parent: "LoopA", Child: "LoopB"}
      - {Parent: "LoopB", Child: "LoopA"}
    expected_exception: "GraphValidationError"
    expected_message_part: "Graph contains a cycle starting from node:"