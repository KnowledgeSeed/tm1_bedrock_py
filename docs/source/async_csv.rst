.. role:: python(code)
   :language: python

.. role:: json(code)
   :language: json

.. role:: sql(code)
   :language: sql

======================================================
`async_executor_csv_to_tm1` Parallel Processing Manual
======================================================

.. contents:: Table of Contents
   :depth: 2

------

.. _introduction:

Introduction
============

The :python:`async_executor_csv_to_tm1()` function is a high-performance orchestrator designed for a specific, powerful workflow: **loading multiple CSV files from a directory into a TM1 cube in parallel**.

The core principle is to treat each CSV file as a pre-defined data slice. The executor pairs each file with a corresponding set of TM1 dimension elements, then assigns each pair to a separate worker thread. Each worker then runs the complete `load_csv_data_to_tm1_cube` process on its assigned file, loading it into the correct, designated slice in the target TM1 cube.

This architecture is ideal for batch processing scenarios where data is delivered as a collection of separate, slice-based files (e.g., one file per region, one file per month).

------

.. _how_it_works:

How It Works: The File & Parameter Pairing Mechanism
====================================================

The function's parallelization strategy is based on a direct pairing of files and TM1 parameters. It is crucial to understand this mechanism to use the function correctly.

1.  **Discover CSV Files**: The function begins by scanning the specified :python:`source_directory` and creating a sorted list of all files ending in `.csv`.

2.  **Generate Parameter Tuples**: Concurrently, it executes the MDX set queries provided in :python:`param_set_mdx_list` to generate a list of unique element combinations (tuples).

3.  **Pair Files with Parameters**: The function then **`zip`s** the list of file paths and the list of parameter tuples together. This creates a one-to-one pairing:
    - The first CSV file in the list is paired with the first parameter tuple.
    - The second CSV file is paired with the second parameter tuple, and so on.

4.  **Assign Unique Work**: Each worker thread is assigned one of these pairs. It uses its assigned file path as the data source and its assigned parameter tuple to populate the `data_mdx_template` (for metadata) and the `clear_param_templates` (for clearing its target slice).

.. warning::
   **Prerequisite: Files and Parameters Must Match!**

   For this process to be logical and correct, the number of CSV files found in the directory and their alphabetical order **must intentionally match** the number and order of the parameter tuples generated by `param_set_mdx_list`. A mismatch will result in data being loaded to the wrong slice or some files being skipped.

------

.. _parameter_reference:

Parameter Reference
===================

Below is a complete reference for the function's parameters.

tm1_service
  *(required)* An active `TM1Service` object, used for initial setup (e.g., fetching parameter elements).

target_cube_name
  *(required, string)* The name of the destination cube in TM1.

source_directory
  *(required, string)* The full path to the directory containing the source CSV files to be loaded.

param_set_mdx_list
  *(required, list[string])* A list of MDX set queries that define the parameters for slicing the target cube. The number of parameter tuples generated from this list should correspond to the number of CSV files.

data_mdx_template
  *(required, string)* An MDX query template used solely for **metadata inference** by the underlying `load_csv_data_to_tm1_cube` function. It should contain placeholders (e.g., `$Period`) that will be populated by the parameters for each worker. It is **not** used to extract data.

data_copy_function
  *(optional, callable)* The function to be executed by each worker. Defaults to :python:`bedrock.load_csv_data_to_tm1_cube`.

clear_param_templates
  *(optional, list[string])* A list of MDX set templates. For each worker, these templates are populated with the worker's specific parameters to generate a unique `target_clear_set_mdx_list`. This ensures each worker clears its own target slice before loading its assigned CSV file.

max_workers
  *(optional, int; default=8)* The number of parallel worker threads to execute. This is the primary performance tuning parameter.

shared_mapping / mapping_steps
  *(optional)* The standard mapping and transformation dictionaries, which are passed through to each worker's `load_csv_data_to_tm1_cube` call.

**kwargs
  *(optional)* Additional keyword arguments to be passed down to each worker. This is the mechanism for providing CSV parsing parameters like :python:`delimiter`, :python:`decimal`, and TM1 writing options like :python:`async_write`.

------

.. _example_workflow:

Example Workflow
================

Assume a directory `C:/data/inbox/` contains two files: `sales_202401.csv` and `sales_202402.csv`.

.. code-block:: python

    import asyncio
    from TM1_bedrock_py import bedrock

    # 1. Define the parameters to match the files in the correct order
    params = ["{[Period].['202401', '202402']}"]

    # 2. Define clear templates that use the Period parameter
    clear_tmpl = ["{[Version].[Forecast]}", "{[Period].[$Period]}"]

    # 3. Define a metadata MDX template
    mdx_meta_tmpl = "SELECT FROM [Sales] WHERE ([Period].[$Period])"

    # 4. Run the executor
    asyncio.run(bedrock.async_executor_csv_to_tm1(
        tm1_service=tm1_connection,
        target_cube_name="Sales",
        source_directory="C:/data/inbox/",
        param_set_mdx_list=params,
        data_mdx_template=mdx_meta_tmpl,
        clear_param_templates=clear_tmpl,
        max_workers=2,

        # Pass-through kwargs for the underlying load_csv_data_to_tm1_cube function
        delimiter=",",
        decimal="."
    ))

------

.. _developer_comments:

Developer Comments & Performance Tuning
=======================================

.. note::
   **Performance Tuning**

   The optimal number of workers depends on the TM1 server's capacity for handling concurrent **write** operations. While TM1 can handle many concurrent reads, write operations often create locks that can serialize the workload. A good starting point is between 2 and 8 workers. Monitor TM1 performance logs (e.g., `tm1top.log`) to identify any cube locking or contention that might limit scalability.

   Default value is `use_blob=False` as `True` needs administrator privileges. Setting the value to `True` improves performance significantly.

.. warning::
   **Thread Safety and Connection Sharing**

   This executor passes the same `tm1_service` object to all worker threads. For many scenarios, this is sufficient. However, for very high-concurrency writes, the most robust pattern is to ensure each thread has its own dedicated TM1 connection to avoid potential locking or session management issues. Consider adapting this executor to use a connection factory pattern if you encounter contention at very high `max_workers`.

.. danger::
   **File and Parameter Mismatch**

   This is the most critical prerequisite for using this function. A mismatch between the number or order of your CSV files and the number or order of your TM1 parameter tuples **will lead to data corruption**, as files will be loaded into the wrong cube slices. Always validate your source directory and `param_set_mdx_list` carefully before execution.

------

.. _conclusion:

Conclusion
==========

The :python:`async_executor_csv_to_tm1()` function is a specialized and powerful tool for high-throughput batch loading scenarios where data is delivered as a collection of pre-sliced files.

By pairing individual data files with specific TM1 target slices defined by MDX parameters, it provides an efficient and scalable way to automate the ingestion of pre-divided datasets. When the prerequisites are met, this executor can dramatically reduce the time required to load large volumes of file-based data into your TM1 cubes.